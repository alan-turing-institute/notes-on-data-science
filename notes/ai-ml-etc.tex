\documentclass[10pt, a4, twocolumn]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{microtype}
%%
\usepackage[margin=0.5in]{geometry}
\usepackage[medium]{titlesec}
%\setlength{\parindent}{1em}
%\setlength{\parskip}{\smallskipamount}
%%
\usepackage{hyperref}
%\usepackage[citestyle=verbose-ibid]{biblatex}
%\addbibresource{references.bib}
%%
\newcommand{\defn}[1]{\textbf{#1}}
\newcommand{\ie}{i.e.\relax}
%%
%% Artifical Intelligence, Machine Learning, and all that
%% This note was written by James Geddes
%%
\title{The Problems of AI \\ === DRAFT v0.1 ===}
\author{James Geddes}
\begin{document}\maketitle
\thispagestyle{empty}\pagestyle{empty}
\begin{quote}
``Intelligence is whatever machines haven't done yet.'' ---Lawrence Tesler
\end{quote}
There is as yet no artifical system that posesses \emph{general}
intelligence. However, there certainly exist many weak artificial intelligences:
`weaker' in the sense of being specialised to particular kinds of problems. It
is interesting to ask what characterises those problems that are amenable to
attack by AI as it exists today.

The examples of which one might be aware seem, on the face of it, to be diverse
and unrelated. Among other things, we have programs which recognise images or
speech, translate from one language to another, learn to play chess, go, or
StarCraft, reason in formal logic, make certain kinds of medical diagnosis,
recommend films, or plan a route of a thousand miles. More recently, finding the
three-dimensional shape of a protein has been added to the list. My sense is
that it is the apparent lack of connection between these examples which gives
the impression that AI is more generally capable than it really is.

On the other hand, there are many examples of things that computers (still)
can't do. Usually these seem more prosaic: schedule a meeting at a time
convenient for colleagues; organise my household finances; solve my customer
service problem; diagnose most medical problems.

What is the connection between the things computers can do? What is the
difference with the things that they can't do? The traditional categories don't
seem to be helpful here. Just from knowing the distinction between `supervised
learning' and `unsupervised learning,' I find it hard to tell that chess is
possible and calendars aren't. Instead, I think it is more useful to focus on a
slightly more abstract description of the \emph{kind} of problem being
solved. In the rest of this note I will try to describe some of these kinds of
problems. I will argue that the kinds of problems that are today amenable to AI
are those which admit a simple and straightforward \emph{description}. In
particular, it is necessary to have a clear question, a clear way of telling
when you have an answer, and often a way to tell when the answer is right. 

Many problems are \emph{not} so easily describable. In some cases, indeed, we
succeeded only by finding this description. Finding a represention of a
real-world problem that is `understandable' by a computer seems to be a large
part of the challenge of applying AI.\footnote{Perhaps this explains the
phenomenon of the quote at the top: one makes progress in part by figuring out
what the problem is; after that, the solution seems much less magical.}

The field of AI is extremely broad. To some extent, it might best be thought of
as a number of different, related fields. The discussion here will be
opinionated and abridged. I will not be concerned with the specifics of
particular algorithms, not with the niceties that distinguish one field from
another. Comments and corrections on this draft are very welcome.

This note is still in draft form. 

\section{What is a problem anyway?}

Imagine a computer program which seaches through all the stars in our galaxy,
looking for ones that are suitable to support life. You give a star to the
computer and the computer tells you `yes' or `no.' But of course, you can't
`give' a star to a computer; instead, you must provide the computer with a
`representation' of star. This representation might be a certain set of
characteristics of the star. For example one might consider the nine numbers
describing the star's mass, brightness, colour, size, rotational speed, position
on the main sequence, and three-dimensional position relative to the galactic
centre. To the computer, a star just \emph{is} this set of nine numbers; and
every possible star (to the computer) is completely described by some set of
numbers. (But note that not every set of nine numbers describes a physically
possible star.)

A typical AI problem is similar to this example. In general, one has a way of
representing each of a very large number of possibilities (of something). The
goal is usually to find---or to say something about---certain, special,
possibilities. In the example, there are many sets of nine numbers, and the
challenge is to identify particular sets that describe stars that might have a
planet capable of sustaining life.

Where problems differ is in what is known about the possibilities and in the
nature of the goal.\footnote{It is always necessary to have a way to represent
each possibility. Even that step can be hard to achieve.} I shall describe four
variations: (1) In \emph{good old-fashioned AI problems} one knows everything:
how to enumerate the possibilities and how to evalate a candidate answer. The
problem is simply that there are too many possibilities to search them all. (2)
In \emph{machine learning problems}, we are given a set of example answers out
of all possibilities. We don't know how, theoretically, to recognise an answer
so instead we spot answers by their `similarity' to one or more of the known
examples. (3) In \emph{inference problems}, we have a collection of theories
about how the world works and how observations are made and we'd like to know
the most likely theory. (4) Finally, in \emph{simulation problems} we have a
theory of how the world works and we want to `run the world forward.'

\section{Four kinds of problem}

\subsection{`Good Old-Fashioned AI' Problems}

One of the first applications of AI was to games such as noughts-and-crosses
(\emph{a.k.a.}, tic-tac-toe). In noughts-and-crosses, there is a universe of
`possible games'---legal sequences of moves eventually leading to a position that
is a win, a loss, or a draw. This universe is unambiguous: a few items of data
suffice to describe any possible position. Furthermore, one could in principle
mechanically enumerate \emph{all} the possibilities, if only one had enough
time. Finally, it is easy to recognise when one has reached a winning position
because that is part of the rules (`three in a row'). Given knowledge of all
possible games starting from a given position, it is slightly harder, but not
much harder, to recognise a winning move from that position.

Noughts-and-crosses is thus an example of a particularly straightforward class
of problems.\footnote{It is the description of the problem that is
straightforward, not the process for finding an answer in reasonable time!}
There is given a collection of `possible solutions,' each of which can be
described completely and unambiguously; and all of which could in principle be
mechanically enumerated. Among these possible solutions there is at least one
`goal solution,' and we know how to recognise the goal should we be given
it. The task is simply to find the goal solution.

There are variations on this kind of problem. One can codify mathematical logic
in such a way as to allow mechanical enumeration of all possible logical
sentences. Given rules that govern the production of allowed sentences from
given ones (\ie, proofs) we can cast purely logical reasoning as a similar
process of search.

Some variations deal with continuous quantities rather than discrete ones. The
classical problem of `function maximisation' is similar: given a mathematical
function of many arguments, find the particular choice of arguments which
maximises the function. Here is another one: given a network of roads and
travel-times, find a route through the network which minimises travel time.

The challenge here, of course, is the sheer number of possibilities. For each
move in a game, there are several possible replies; to each of those, several
further replies; and so on. The resulting so-called `combinatorial explosion'
soon exceeds the computational power of any computer (certainly for chess, if
not for noughts-and-crosses).

The kinds of problems that were explored in the early years of AI were of this
ilk. For many of these kinds of problems, we now have methods of finding
solutions that are `good enough', even if not perfect. If your problem is one of
these, then AI might work for you. On the other hand, if your central problem
\emph{is} like this then you likely know it already and are already aware of the
various approaches. 

\subsection{Machine Learning Problems}

Good old-fashioned AI both succeeded and failed. It succeeded in the sense that
your satnav finds a shortest route out of a huge number of possibilities. But it
failed because in many real-world problems we are \emph{unable} precisely to
codify the universe of possibilities. These real-world problems include such
things as image and speech recognition, `common sense reasoning,' and
perception. The problem seems to be that a simplistic approach to symbolic
reasoning is `too fragile' when it comes to the messiness of the real world:
there are too many rules, too many exceptions to the rules, and too many
exceptions to the exceptions.

As an example, consider the problem of recognising a handwritten digit from an
image. An image can be represented as a (large) collection of numbers: namely,
the darkness of each pixel in the image. However, not all collections of numbers
correspond to pictures of digits. It turns out to be very hard to write down a
precise set of rules describing the shape of a `7,' and how it differs from a
`1,' across all different kinds of handwriting. (The problem is even more actute
in speech-to-text, or machine translation.)

What categorises these problems is that we don't know, theoretically, how to
recognise a solution even if we have one. We don't have a mechnical way to tell
a `7' from a `1'---that's the whole problem.

So in machine learning we ask a different question. We give up on the plan of
trying to compute an answer from `first principles.' Instead, we observe that
sometimes it is possible to obtain many \emph{exemplar} answers to the very same
problem. For example, we might ask a human to label images of handwritten digits
with the digit itself in machine-readable
form.\footnote{\url{http://yann.lecun.com/exdb/mnist/}} Now, given a new image,
we can ask, to which of the exemplar images is it `closest'? Having found the
`nearby' examples, we guess that the digit represented by the new image is the
same as these. Thus we have changed the question from `what digit does this
image represent?' to `to which example images is this one most similar?'

More generally, we have a set of `observations,' each of which is completely
described by a set of numbers called \emph{features}.\footnote{A feature doesn't
have to be a number but that's not important for this discussion.}  We have a
set of example observations, known as the \emph{training data} and we'd like to
predict some or all features of a new observations, possibly based on knowledge
of some other features of that new observation. To do so, we find `similar'
observations in the training data, and predict, for our new observation, the
same as the features of those similar ones.

Why is \emph{this} hard? For one thing, we've acquired a new problem: what is
meant by `close'? In the example of an image of a digit, we would want to say
that images are `similar' if they differ by a small rotation, or a change of
scale, or a line being thicker or moved slightly. It turns out that a certain
kind of `neural network' is very good at compensating for just \emph{these}
kinds of differences and that is why we now have very good image
recognition.\footnote{It's important to recognise that image recognition is a
prediction of a \emph{label} associated with an image: the system in no way
understands the nature of the objects \emph{in} the image. There exist examples
of images that are wildly misclassified (to a human) because the meaning of
`close' imposed by the algorithm is merely about pixels; not about real things.}
But for other problems, different notions of similarity will be appropriate.

Still, one could have hoped that, with sufficient data, we would not need to
make so many assumptions. However, the amount of data that constitutes `enough,'
in this sense, increases exponentially with the number of features\footnote{That
is, adding a feature \emph{multiplies} the amount of data required by some
amount.} and so, in practice, there isn't enough.

Moreover, one is often interested in predicting the result of making a
\emph{change} to the world, rather than simply predicting what would happen
under ordinary circumstances. Unless one has made that very change in training
data (for example, by running a controlled trial) then one will be attempting to
predict what happens a long way from any example.

Thus it is typically necessary to make assumptions---all machine learning models
make assumptions, either explicitly or implicitly.\footnote{One way to make
assumptions is by choosing a particular kind of algorithm, the details of which
impose a certain way of looking at the world. Alternatively, one can pre-compute
certain functions of the features and give those to the model those
instead. This is known as `feature engineering;' the idea is to compute `useful'
features, ones that capture something we know to be important about the world. A
third way is to explicitly include in the algorithm a `model of how the world
works.'}

\subsection{Inference Problems}

Machine learning models necessarily make assumptions, usually about what it
means for observations to be `similar' to each other. But those assumptions tend
to arise implicitly, from the structure of the model. It can be hard to see what
is actually implied by a choice of one model over another.

Sometimes, however, we actually do have a good idea of how the world works. That
is, we have a model of how the observational data arise, based on some
underlying theory, although we may not have full knowledge of parts of that
theory. In this case we would want the assumptions made by our model to be
consistent with how the world really is. For example, consider trying to track
the infection rate of a new virus. There are reasonable models of how viruses
spread within populations. If one has some idea of how many people were infected
each week for the past few weeks, one already has a good guess, from the model,
as to what the infection rate will be this week. One can combine that `prior
view' with the new data coming in daily to provide updated estimates.

This is the idea behind Bayesian inference: we have a view of how the world is
but we are uncertain about parts of it. We make some observations. Based on
those observations, we update our view of the world, become more certain about
how it is.

What is nice about Bayesian inference is that it is principled: if we have a
theory of the world we can adapt the approach to that theory. Furthermore, we
get an estimate of the degree of certainty we have in any estimates. (On the
other hand, it can be computationally very expensive.)

\subsection{Simulation Problems}

Finally, simulation is well-known method for predicting the future. In problems
of this nature, we have a complete theory of how the world evolves in time,
along with knowledge of how the world is now, and we wish to `run the world
forward,' starting from a particular configuration, to see what will happen.

Sometimes we can't nail down how the world is right now; we only know that it is
in one of a set of possible configurations. In that case, we might try to
simulate many possibilities and ask what can be said of all the results.

Problems here are typically engineering or physics problems. Sometimes, however,
a system can be considered to be made up of a large number of similar entities,
all evolving quasi-independently. Then we have an `agent-based'
model. Agent-based models have been used to simulate the spread of SARS-CoV-2,
for example.

\section{The Future}

TBD. PPL? GOFAI+ML? 


\subsubsection{State-of-the-art}

Some advances in recent years have come from combining machine learning with
traditional AI.\@ The rough idea is that, when searching for a particular
possibility, we can use a machine learning model to suggest likely avenues of
exploration. This is what AlphaGo does, for example: it uses a machine learning
model, trained on lots of example games, to narrow down the search to
potentially good moves.


\section{Summary}

This note describes four classes of problems that can be addressed by AI as it
stands today. All problems require a precise representation of a certain
collection of possibilities; they differ in what else is known. If your problem
can be made to conform to these descriptions then AI may well be able to help.

\begin{enumerate}
\item \emph{GOFAI:} All possible possibilities can be enumerated
  mechanically. One or more possibilities constitute a solution state and these
  possibilities are straightforward to describe. We wish to find the solution
  state(s).
\item \emph{Machine Learning:} There is a precise description of all
  possibilities but not all of these are realistic. Although the realistic
  possibilities are not straightforward to describe there are many
  examples. Given partial information about a particular possibility, we wish to
  find examples that are similar to it in order to guess the remaining details.
\item \emph{Inference:} There is a precise description of all possibilities and
  a theory describing what observations would be made if a particular one
  happened to be true. The theory is not completely known: there are parameters
  about which we are uncertain, or perhaps competing theories with varying
  degrees of plausibility. However, we have observational data. We wish to
  recover as much information as possible about which model of the world is most
  likely from this observational data.
\item \emph{Simulation:} We have a complete model of how the world changes in
  time and information about how it is now. We wish to say what the world will
  look like later.
\end{enumerate}

What does this leave? TBD.

\newpage
\section{OLD}

\subsection{History}

We seem to be in the middle of the third golden age of AI, at least as measured
by research funding and public interest in `machine learning.'\footnote{The
first age (roughly, `good old-fashioned AI') lasted from the 1950s until perhaps
the Lighthill report of 1974. The second---continuing the themes of the first but
having logical reasoning, among other things, as a domain of application---lasted
from about 1980 until the end of the Alvey Programme, the end of the Fifth
Generation Computing Project, and the collapse of the Lisp Machine market, all
towards the end of that decade. See, for example,
\url{https://en.wikipedia.org/wiki/History_of_artificial_intelligence}. I am
omitting a great deal of history. In particular, the ideas behind `deep
learning' were formed in the early days of AI; and the ideas behind
probabilistic inference arguably date back to the 18th century. I'm also
ignoring, \emph{inter alia}, embodied cognition, various models of knowledge
representation, and fuzzy logic.} In each of these ages---and also during the `AI
winters' which separated them---there were genuine advances in AI: in planning and
search, automated reasoning and theorem proving, expert systems, image and
speech recognition, and others. Yet there were also many unrealistic
expectations of the breadth of application of these breakthroughs. I think it is
worth trying to understand what it is that made particular problems
tractable.

\end{document}
