\documentclass[10pt, a4paper, twocolumn]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{microtype}
%%
\usepackage[margin=0.5in]{geometry}
\usepackage[medium]{titlesec}
%\setlength{\parindent}{1em}
%\setlength{\parskip}{\smallskipamount}
%%
\usepackage{hyperref}
%\usepackage[citestyle=verbose-ibid]{biblatex}
%\addbibresource{references.bib}
%%
\newcommand{\defn}[1]{\textbf{#1}}
\newcommand{\ie}{i.e.\relax}
%%
%% Artifical Intelligence, Machine Learning, and all that
%% This note was written by James Geddes
%%
\title{The Problems of AI \\ === DRAFT v0.1 ===}
\author{James Geddes}
\begin{document}\maketitle
\thispagestyle{empty}\pagestyle{empty}
\begin{quote}
``Intelligence is whatever machines haven't done yet.'' ---Lawrence Tesler
\end{quote}
There is as yet no artifical system that posesses \emph{general}
intelligence. However, there certainly exist many weak artificial intelligences:
`weaker' in the sense of being specialised to particular kinds of problems. It
is interesting to ask what characterises those problems that are amenable to
attack by AI as it exists today.

The examples of which one might be aware seem, on the face of it, to be diverse
and unrelated. Among other things, we have programs which recognise images or
speech, translate from one language to another, learn to play chess, go, or
StarCraft, reason in formal logic, make certain kinds of medical diagnosis,
recommend films, or plan a route of a thousand miles. More recently, finding the
three-dimensional shape of a protein has been added to the list. My sense is
that it is the apparent lack of connection between these examples which gives
the impression that AI is more generally capable than it really is.

On the other hand, there are many examples of things that computers (still)
can't do. Usually these seem more prosaic: schedule a meeting at a time
convenient for colleagues; organise my household finances; solve my customer
service problem; diagnose most medical problems.

What is the connection between the things computers can do and what is the
difference with the things that they can't do? The traditional categories don't
seem to be helpful here. I find it hard to tell, from knowing the distinction
between `supervised' and `unsupervised' learning, that chess is possible and
calendars aren't. Instead, I think it is more useful to focus on a simpler,
slightly more abstract description of the \emph{kind} of problem being
solved. In the rest of this note I will try to describe some of these kinds of
problems. I will argue that the kinds of problems that are today amenable to AI
are those which admit a simple and straightforward \emph{description}; in
particular, those for which the question is clear, it's clear when you have an
answer, and it's clear when the answer is right.

In some cases, indeed, we succeeded only by finding this description. Finding a
represention of a real-world problem that is `understandable' by a computer
seems to be a large part of the challenge of applying AI and perhaps this
explains the phenomenon of the quote at the top: one makes progress in part by
figuring out what the problem is; after that, the solution seems much less
magical.

The field of AI is extremely broad. To some extent, it might best be thought of
as a number of different, related fields. The discussion here will be
opinionated and abridged. I will not be concerned with the specifics of
particular algorithms, not with the niceties that distinguish one field from
anohter. Comments and corrections on this draft are very weclome.


\section{What is a problem anyway?}

Imagine a computer program which seaches through all the stars in our galaxy,
looking for ones that are suitable to support life. You give a star to the
computer and the computer tells you `yes' or `no.' Of course, you can't `give' a
star to a computer; instead, you must provide the computer with a
`representation' of star. This representation might be a certain set of
characteristics of the star. For example one might consider nine numbers
describing the star's mass, brightness, colour, size, rotational speed, position
on the main sequence, and three-dimensional position relative to the galactic
centre. To the computer, a star just \emph{is} this set of nine numbers; and
every possible star (to the computer) is completely described by some set of
numbers.

A typical AI problem is similar to this example. In general, one has a way of
representing each of a very large number of possibilities (of something). The
goal is typically to find---or to say something about---certain, special,
possibilities. In the example, there are many sets of nine numbers, and the
challenge is to identify particular sets that describe stars that might have a
planet capable of sustaining life.

Where problems differ is in what is known about the possibilities (more than how
to represent them, which must always be known) and in the nature of the goal. I
shall describe four variations: (1) In \emph{good old-fashioned AI problems} one
knows everything: how to enumerate the possibilities and how to evalate a
candidate answer. (2) In \emph{machine learning problems}, we are given a set of
example answers and we have a view on what it means for two possibilities to be
`similar;' we recognise an answer by its similarity to one or more of a set of
known examples. (3) In \emph{inference problems}, we have a collection of
theories of how the examples got here and we'd like to know which is more likely
to be correct. (4) Finally, in \emph{simulation problems} we have a theory of
how the world works and we want to `run the world forward.'

\section{Four kinds of problem}

\subsection{`Good Old-Fashioned AI' Problems}

One of the first applications of AI was to games such as noughts-and-crosses
(\emph{a.k.a.}, tic-tac-toe). In noughts-and-crosses, there is a universe of
`possible games'---legal sequences of moves eventually leading to a position that
is a win, a loss, or a draw. This universe is unambiguous: a few items of data
suffice to describe a possible position. Furthermore, one could in principle
mechanically enumerate \emph{all} the possibilities, if only one had time
enough. Finally, it is easy to recognise when one has reached a winning position
because that is part of the rules (`three in a row'). Given knowledge of all
possibly games from a given position, it is slightly harder, but not much
harder, to recognise a winning move.

Noughts-and-crosses is thus an example of a particularly straightforward class
of problems. There is given a collection of `possible solutions,' each of which
can be described completely and unambiguously; and all of which could in
principle be mechanically enumerated. Among these possible solutions there is at
least one `goal solution,' and we know how to recognise the goal should we be
given it. The task is simply to find the goal solution.

There are variations on this kind of problem. One can codify mathematical logic
in such a way as to allow mechanical enumeration of all possible logical
sentences. Given rules that govern the production of allowed sentences from
given ones (\ie, proofs) we can cast purely logical reasoning as a similar
process of search.

Some variations deal with continuous quantities rather than discrete ones. The
classical problem of `function maximisation' is similar: given a mathematical
function of many arguments, find the particular choice of arguments which
maximises the function. Here is another one: given a network of roads and
travel-times, find a route through the network which minimises travel time.

The challenge here, of course, is the sheer number of possibilities. For each
move in a game, there are several possible replies; to each of those, several
further replies; and so on. The resulting so-called `combinatorial explosion'
soon exceeds the computational power of any computer (certainly for chess, if
not for noughts-and-crosses).

The kinds of problems that were explored in the first and second phases of AI
were of this ilk. For many of these kinds of problems, we now have methods of
finding solutions that are `good enough', even if not perfect. If your problem
is one of these, then AI might work for you. On the other hand, if your central
problem is like this then you likely know it and are already aware of the
various approaches. In this case, it might be interesting to look at other
problems you are facing and to ask whether it could be possible to cast them
into this form: it might be that you have challenges that could not be solved
ten years ago which now can, just because computers are faster. You will find
that the hard part is converting a real-world problem to a mathematical one.


\subsection{Machine Learning Problems}

The first kind of AI both succeeded and failed. It succeeded in the sense that
your satnav finds a shortest route out of a huge number of possibilities. But it
failed because in many real-world problems we are unable precisley to codify the
universe of possibilities. These real-world problems include such things as
image and speech recognition, `common sense reasoning,' and perception. The
problem seemed to be that GOFAI was too fragile when it came to the messiness of
the real world: there were too many rules, too many exceptions to the rules, and
too many exceptions to the exceptions.

As an example, consider the problem of recognising a handwritten digit from an
image. We can still partly answer two of the three questions above. An image can
be represented as a (large) collection of numbers: namely, the darkness of each
pixel in the image. However, not all collections of numbers correspond to
pictures of digits; and it turned out to be very hard to write down a precise
set of rules describing the shape of a `7,' and how it differs from a `1,'
across all different kinds of handwriting. (The problem is even more actute in
speech-to-text, or machine translation.)

What categorises these problems is the lack of answer to the third question: we
don't know how to recognise a solution even if we had one. We don't have a
mechnical way to tell a `7' from a `1'---that's the whole problem.

In machine learning we ask a different question. We give up on the plan of
trying to compute an answer from `first principles.' Instead, we observe that
sometimes it is possible to obtain many \emph{exemplar} answers to the very same
problem. For example, we might ask a human to label images of handwritten digits
with the digit itself in machine-readable
form.\footnote{\url{http://yann.lecun.com/exdb/mnist/}} Now, given a new image,
we can ask, to which of the exemplar images is it `closest'? Having found the
`nearby' examples, we guess that the digit represented by the new image is the
same as these. Thus we have changed the question from `what digit does this
image represent?' to `to which example images is this one most similar?'

More generally, we have a set of `observations,' each of which is completely
described by a set of numbers called \emph{features}.\footnote{A feature doesn't
have to be a number but that's not important for this discussion.}  We have a
set of exemple observations, known as the \emph{training data} and we'd like to
predict some or all features of a new observations, possibly based on knowledge
of some other features of that new observation. To do so, we find `similar'
observations in the training data, and predict, for our new observation, the
same as the features of those similar ones.

Why is \emph{this} hard? For one thing, we've acquired a new problem: what is
meant by `close'? In the example of an image of a digit, we would want to say
that images are `similar' if they differ by a small rotation, or a change of
scale, or a line being thicker or moved slightly. It turns out that a certain
kind of `neural network' is very good at compensating for just \emph{these}
kinds of differences and that is why we now have very good image
recognition.\footnote{It's important to recognise that image recognition is a
prediction of a \emph{label} associated with an image: the system in no way
understands the nature of the objects \emph{in} the image. There exist examples
of images that are wildly misclassified (to a human) because the meaning of
`close' imposed by the algorithm is merely about pixels; not about real things.}
But for other problems, different notions of similarity will be appropriate.

Still, one could have hoped that, with sufficient data, we would not need to
make so many assumptions. However, the amount of data that constitutes `enough,'
in this sense, increases exponentially with the number of features\footnote{That
is, adding a feature \emph{multiplies} the amount of data required by some
amount.} and so, in practice, there isn't enough.

Moreover, one is often interested in predicting the result of making a
\emph{change} to the world, rather than simply predicting what would happen
under ordinary circumstances. Unless one has made that change in training data
(for example, by running a controlled trial) then one will be attempting to
predict what happens a long way from any example.

Thus it is typically necessary to make assumptions---all machine learning models
make assumptions, either explicitly or implicitly.\footnote{One way to make
assumptions is by choosing a particular kind of algorithm, the details of which
impose a certain way of looking at the world. Alternatively, one can pre-compute
certain functions of the features and give those to the model those
instead. This is known as `feature engineering;' the idea is to compute `useful'
features, ones that capture something we know to be important about the world. A
third way is to explicitly include in the algorithm a `model of how the world
works.'}

\subsubsection{State-of-the-art}

Some advances in recent years have come from combining machine learning with
traditional AI.\@ The rough idea is that, when searching for a particular
possibility, we can use a machine learning model to suggest likely avenues of
exploration. This is what AlphaGo does, for example: it uses a machine learning
model, trained on lots of example games, to narrow down the search to
potentially good moves.

\subsection{Inference Problems}

Machine learning models necessarily make assumptions, usually about what it
means for observations to be `similar' to each other. But those assumptions tend
to arise implicitly, from the structure of the model; it can be hard to see what
is actually implied by a choice of one model over another.

Sometimes, however, we actually do have a good idea of how the world works. That
is, we have a model of how the observational data arise, based on some
underlying theory, although we may not have full knowledge of parts of that
theory. In this case we would want the assumptions made by our model to be
consistent with how the world really is. For example, consider trying to track
the infection rate of a new virus. There are reasonable models of how viruses
spread within populations. If one has some idea of how many people were infected
each week for the past few weeks, one already has a good guess, from the model,
as to what the infection rate will be this week. One can combine that `prior
view' with the new data coming in daily to provide updated estimates.

This is the idea behind Bayesian inference: we have a view of how the world is
but we are uncertain about parts of it. We make some observations. Based on
those observations, we update our view of the world, become more certain about
how it is.

What is nice about Bayesian inference is that it is principled: if we have a
theory of the world we can adapt the approach to that theory. Furthermore, we
get an estimate of the degree of certainty we have in any estimates. (On the
other hand, it can be computationally very expensive.)

\subsection{Simulation Problems}

Finally, simulation is well-known method for predicting the future. In problems
of this nature, we have a complete theory of how the world evolves in time,
along with knowledge of how the world is now, and we wish to `run the world
forward,' starting from a particular configuration, to see what will happen.

Sometimes we can't nail down how the world is right now; we only know that it is
in one of a set of possible configurations. In that case, we might try to
simulate many possibilities and ask what can be said of all the results.

Problems here are typically engineering or physics problems. Sometimes, however,
a system can be considered to be made up of a large number of similar entities,
all evolving quasi-independently. Then we have an `agent-based'
model. Agent-based models have been used to simulate the spread of SARS-CoV-2,
for example.

\section{The Future}

TBD. PPL? GOFAI+ML? 

\section{Summary}

This note describes four classes of problems that can be addressed by AI as it
stands today. All problems require is to specify precisely a certain collection
of configurations; they differ in what else is known.

\begin{itemize}
\item{GOFAI} All possible configurations can be enumerated mechanically. One or more
  configurations consisute a solution state and this configuration is
  straightforward to describe. We wish to find the solution state.
\item{Machine Learning} There is a precise description of all configurations but not all of these
  are possible or realistic. Although the possible configurations are not
  straightforward to describe there are many examples of such possible
  configurations. Given partial information about a particular configuration, We
  wish to find examples that are similar to it in order to guess the remaining details.
\item{Inference} There is a precise description of all configurations and a
  theory describing what observations would be made under a particular possible
  configuration. The theory is not completely known: there are parameters about
  which we are uncertain, or perhaps competing theories with varying degrees of
  plausibility. However, we have observational data from particular
  configurations. We wish to recover as much information as possible about which
  model of the world is most likely from observational data.
\item{Simulation} We have a complete model of how the world changes in time and
  information about how it is now. We wish to say what the world will look like later.
\end{itemize}

\newpage
\section{OLD}

\subsection{History}

We seem to be in the middle of the third golden age of AI, at least as measured
by research funding and public interest in `machine learning.'\footnote{The
first age (roughly, `good old-fashioned AI') lasted from the 1950s until perhaps
the Lighthill report of 1974. The second---continuing the themes of the first but
having logical reasoning, among other things, as a domain of application---lasted
from about 1980 until the end of the Alvey Programme, the end of the Fifth
Generation Computing Project, and the collapse of the Lisp Machine market, all
towards the end of that decade. See, for example,
\url{https://en.wikipedia.org/wiki/History_of_artificial_intelligence}. I am
omitting a great deal of history. In particular, the ideas behind `deep
learning' were formed in the early days of AI; and the ideas behind
probabilistic inference arguably date back to the 18th century. I'm also
ignoring, \emph{inter alia}, embodied cognition, various models of knowledge
representation, and fuzzy logic.} In each of these ages---and also during the `AI
winters' which separated them---there were genuine advances in AI: in planning and
search, automated reasoning and theorem proving, expert systems, image and
speech recognition, and others. Yet there were also many unrealistic
expectations of the breadth of application of these breakthroughs. I think it is
worth trying to understand what it is that made particular problems
tractable.

\end{document}
