\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{beton}
\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{microtype}
\usepackage{booktabs}
% \usepackage[medium, compact]{titlesec}
%\usepackage[inline]{asymptote}
%\usepackage{tikz-cd}
\DeclareFontSeriesDefault[rm]{bf}{sbc}
% \usepackage{amssymb}
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\title{Increasingly tricky optimisation problems}
\author{James Geddes}
\date{\today}
%%
\DeclareBoldMathCommand{\setR}{R}
\DeclareBoldMathCommand{\bfC}{C}
\DeclareBoldMathCommand{\bfG}{\Gamma}
\newcommand{\id}{\mathbold{1}} 
\newcommand{\bzero}{\mathbold{0}} % I don't know why \bm{0} fails.
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\nullspace}{null}
\newcommand{\eg}{\emph{Example:}}
\newcommand{\ie}{\emph{i.e.}}
\hyphenation{anti-sym-met-ric}
\begin{document}
\maketitle

We consider a series of increasingly tricky minimisation problems. 

\section*{Notation}

For $V$ a vector space, we write $v^a$ to represent an element of
$V$.

The superscript, in this case “$a$,” should be a roman letter towards
the beginning of the alphabet. Its meaning is simply that $v^a$ is a
vector; it does \emph{not} denote that $v^a$ is the $a$th component of a
vector in some basis. The expression $v^b$ is the same
vector. Similarly, $w_a$, with the index lowered, denotes an element
of the dual space, $V^*$. This notation, due to Penrose, is known as
\emph{abstract index notation}.

There are other ways of notating an element of some vector space. A
typical approach in a mathematics text is to write simply $v\in V$, with
no adornment of the symbol. One must then mentally keep track of which
symbols are vectors and which are numbers (or other things). A
physicist might instead boldface the vector, as in
$\mathbold{v}\in V$, or perhaps otherwise decorate it, as in
$\vec{v}\in V$.

A problem with these other notations is that they do not naturally
extend to other denizens of the vector universe. For example, if one
frequently encounters dual vectors, then the mathematician now has to
remember which symbols are being used for dual vectors as opposed to
vectors. A physicst typically doesn't care about the distinction
between $V$ and $V^*$, but you might catch one writing, for example
$\mathbold{v}^T$. \sidenote*{What does $\mathbold{v}^T$ mean? It
  means, “the matrix representation of that dual vector obtained, from
  the vector $\mathbold{v}$, by means of the isomorphism between $V$
  and $V^*$ generated by some particular basis that I have in mind.”}
And of course there are other spaces whose inhabitants are useful: the
space of linear maps $V\to V$, say, or the space $V\to V^*$ (where
bilinear forms live). How should these be written?

Abstract index notation provides a convenient approach to denoting
these objects. Consider, for example, the dual space, $V^*$, which is
the space of linear maps from $V$ to the reals (or the complexes). A
mathematician might write, say, $w(v)$ to denote the action of
$w\in V^*$ on $v\in V$. That is straightforward, yet some things are
lost. The value $w(v)$ is, by definition, linear in both $w$ and $v$
but that linearity is not apparent from the notation. The action of
$w$ on $v$ can equally well be thought of as the action of $v$ on $w$
but that symmetry is not apparent either.

In abstract index notation, the action of $w_a\in V^*$ on $v^a\in V$ is
written $w_av^a$. That is, we write the $w_a$ and $v^a$ next to each
other, as if we were “multiplying” them, and ensure that the
\emph{same} index is used for both. (The expression $w_av^b$ means
something else.) Now the linearity and symmetry are clearer: although
this expression does \emph{not} denote an ordinary product of numbers,
it looks like one, and therefore looks like it ought to distribute
over addition, in the sense that $w_a(u^a+v^a) = w_au^a +
w_av^a$. Which it does.

\begin{margintable}
  \centering
  \begin{tabular}{@{}lllll@{}}
    \toprule
    Notation & Vect. & Dual & Op. & Contr. \\
    \midrule
    Abstr. ind. & $v^a$ & $w_a$ & $T^a{}_b$ & $v^a w_a$ \\
    \bottomrule
  \end{tabular}
  \caption{Different notations}
  \label{tab:notation}
\end{margintable}


We may have occasion to choose a basis for $V$. In that case, we will
denote the $\mu$th component of $v^a$ in that basis by $v^\mu$. The
distinction is that now we have written a greek index. That is to say,
whereas $v^a$ is a vector, the \emph{Ding an sich}, $v^\mu$ is a number,
one number for each basis. Likewise, by $w_\nu$ we will mean the
$\nu$th element of the dual vector $w_a$ with respect to some basis
on $V^*$ (which will almost always be dual basis to whatever basis we
have chosen for~$V$).

The following highly convenient fact is one reason that this notation
is useful: Fix a basis for $V$ and choose, for a basis of $V^*$, the
dual basis. Then for $v^a\in V$ and $w_b\in V^*$ we have
$w_av^a = \sum_{\mu=1}^{\dim V} w_\mu v^\mu$.

As a matter of history, physicists have typically written vectors
using a basis-dependent representation. As it turns out, physical laws
involve a lot of terms like $\sum_\mu v^\mu w_\mu$. It was decided that it
would make life easier to drop the “$\sum$” and instead follow the
convetion that the summation is implied, so long as some index is
repeated, once “up” and once “down.” Abstract index notation has the
great benefit of looking \emph{just like} the physicists' notation
whilst being coordinate-free.

What about other objects? It turns out that this notation handles
those nicely as well, as long as there aren't too many “individual”
vector spaces around. For example, we denote an element of
$\mathcal{L}(V,V^*)$ by means of two lowered indices, like so:
$Q_{ab}$. To act on some $v^a\in V$ we write $Q_{ab}v^b$. Note that
after we have “paired up” the $b$s, there is only one index “left
free,” and that index, $a$, is lowered. Our convention is that a thing
with a lowered index is an element of $V^*$ and that is indeed what
the action of $Q$ produces.

There's no \emph{content} to the above; it's just notation, albeit a
very suggestive one. It is suggestive because, if one chooses a basis
for $V$ (and the corresponding dual basis for $V^*$), then one obtains
true formulae on replacing roman superscripts and subscripts by greek
ones and adding summations where necessary. For example, suppose
$Q_{\mu\nu}$ is the matrix representation of $Q_{ab}$. Then the
$\mu$th component of $Q_{ab}v^b$ is precisely $\sum_\nu Q_{\mu\nu}v^\nu$.

Abstract index notation has some additional advantages when we come to
take gradients.

\section{Problem 1: A one-dimensional quadratic}

\emph{Problem:} Find
\begin{equation}
  x_\text{min} = \argmin_{x\in\setR} f(x), \quad\text{where $f(x) = \alpha + \beta x + \gamma x^2$}.
\label{problem-quadr-one-d}
\end{equation}
\emph{Solution:} A solution exists if and only if $\gamma>0$; and, in
that case, $x_\text{min} = -\frac{1}{2}\gamma^{-1}\beta$.

Why is that the solution? Here are two ways to approach the
problem. One might “complete the square,” in the term on the right;
that is, write
\begin{equation*}
  f(x) = \gamma\bigl(x + \frac{1}{2}\gamma^{-1}\beta\bigr)^2 - \frac{\beta^2}{4\gamma}. 
\end{equation*}
(To see this reproduces the original expression for $f(x)$, expand the
right-hand side.) Now, the second term on the right is constant and
therefore cannot affect the location of the minimum. And observe that
the first term on the right is strictly positive if and only if
$\gamma>0$ and that in this case its minimum is zero, which it attains
precisely when $x = =\frac{1}{2}\gamma^{-1}\beta$.

Another way to approach the problem is to compute the derivative of
$f(x)$ with respect to $x$ and find the place where it is zero. That
is, we note that
\begin{equation*}
  \left.\frac{df}{dx}\right|_{x = x_\text{min}} = 0.
\end{equation*}
In the general case, there are other places where the derivative might
be zero (namely, at a maximum or point of inflection) and in any case
we may only have located a local minimum, not necessarily the true,
global minimum. Nevertheless, let's try this approach. We have
\begin{equation*}
  \frac{df}{dx} = \beta + 2 \gamma x,
\end{equation*}
whence $x_\text{min} = -\frac{1}{2}\gamma^{-1}\beta$. On this approach, we must
do some extra work to decide whether we have found a minimum. That
extra work is to examine the sign of the second derivative, $d^2f/dx^2$,
which will be strictly positive at a minimum. Indeed, since
\begin{equation*}
  \frac{d^2f}{dx^2} = 2\gamma,
\end{equation*}
we conclude that we have found minimum only when $\gamma> 0$.

Now let's solve the same problem in more than one dimension.

\section{Problem 2: A multi-dimensional quadratic}

\emph{Problem}: Consider the function of two variables, 
\begin{equation*}
  f(x,y) = 1 + x + y + x^2 + y^2 + xy. 
\end{equation*}
For what value of $x$ and $y$ is $f(x,y)$ minimised?\sidenote*{You
  should try to solve this problem!}

Here is a more general version of this problem:

\emph{Problem:} Fix a finite-dimensional, real vector space,~$V$. Let
$\alpha\in\setR$ be a number, $w_a\in V^*$ a dual vector, and
$Q_{ab} \in \mathcal{L}(V,V^*)$ a symmetric, positive-definite, bilinear
form. Find
\begin{equation*}
x^a_\text{min} = \argmin_{x^a\in V} f(x^a),\quad\text{where $f(x^a) = \alpha + w_ax^a + Q_{ab}x^ax^b$}.
\end{equation*}

In some sense, this is the simplest, non-trivial, minimisation
problem. The space (from which we are finding the minimiser) is a
nice, “flat,” vector space, not some arbitrary thing like the surface
of a sphere. The function is perhaps the simplest function that
\emph{has} a minimum, not some general function which wiggles all over
the place. (We can't simplify much futher: a constant function attains
its minimum everywhere; and a linear function has no minimum value.)

\emph{Solution:}
\begin{equation}
  x^a_\text{min} = -\frac{1}{2} {(Q^{-1})}^{ab}w_b.
\label{eq:sol-quadr-vect}
\end{equation}

Before explaining why this is the solution, we describe what it
means. What is the term ${(Q^{-1})}^{ab}$? Ignoring the indices for a
minute, it is the inverse of the map $Q$. Since
$Q\in\mathcal{L}(V, V^*)$, the inverse, $Q^{-1}$, must be an element of
$\mathcal{L}(V^*, V)$ (that is, it goes the other way). In other words,
$Q^{-1}$ acts on an element of the dual space, such as $w_a$, to
produce an element of~$V$. In abstract index notation, that is
expressed by writing $Q^{-1}$ with two raised indices. To say that
$Q^{-1}$ is the inverse of $Q$ is to say that
${(Q^{-1})}^{ac}Q_{cb} = \id^a{}_b$. On the right, $\id^a{}_b$ is the
identity operator on~$V$.

Why is this the solution? By analogy with the one-dimensional case,
there are two approaches. The first is to complete the square:
\begin{equation*}
  f(x^a) = Q_{ab}\Bigl(x^a +
  \frac{1}{2}{(Q^{-1})}^{ac}w_c\Bigr)\Bigl(x^b +
  \frac{1}{2}{(Q^{-1})}^{bc}w_c\Bigr) - \frac{1}{4}w_aw_b{(Q^{-1})}^{ab}.
\end{equation*}
This time, the right-hand side looks rather complicated. However, the
terms in large parentheses are each the same vector. That is to say,
if we write
\begin{equation*}
  \xi^a = x^a +  \frac{1}{2}{(Q^{-1})}^{ac}w_c,
\end{equation*}
(note that all the indices work out!) then $f$ becomes the much
simpler-looking
\begin{equation*}
  f(x^a) = Q_{ab}\xi^a\xi^b + \text{a constant}.
\end{equation*}
Noting that $Q_{ab}$ is positive-definite, we conclude that the
condition for $x^a$ to be a minimiser is that $\xi^a = \bzero$ (the zero
vector) from which eq.~\ref{eq:sol-quadr-vect} follows.

Alternatively, we might try taking the derivative again. For that, we
will need to understand how to take derivatives of functions on a
vector space.

\section*{Gradients}

Here is a coordinate-free version of gradients.

Let's start with the ordinary, one-dimensional case. The definition of
the usual, one-dimensional derivative of some function $f(x)$ at the
point $x$ is:
\begin{equation*}
  f'(x) = \lim_{\delta\to0} \frac{f(x+\delta) - f(x)}{\delta}.
\label{eq:ordinary-derivative}
\end{equation*}

That is, we take the difference in $f$ between its value at $x$ and
its value a small distance away, $x+\delta$, and divide that
by~$\delta$. We are computing “how fast $f$ is changing, with respect to
very small changes in $x$?“ Then we let $\delta$ approach zero. If the
resulting limit exists, then we say the derivative is the limiting
value. (The limit may not exist, in which case $f$ is not
differentiable at~$x$.)

Now we'd like to extend this idea to the case where the function is
defined over a vector space. Of course, the “rate of change” of $f$ as
we change $x$ will, in general, now depend on the \emph{direction} in
which we change~$x$. So, if we want to retain the idea of the
derivative as a slope, we must pick a direction.

Thus, let $\delta^a$ be a vector. The \emph{directional derivative} of $f$
along $\delta^a$ is the limit, if it exists,
\begin{equation*}
  \lim_{\lambda\to0} \frac{f(x^a+\lambda \delta^a) - f(x^a)}{\lambda}.
  \label{eq:directional-derivative}
\end{equation*}
Compare this with eq.~\ref{eq:ordinary-derivative} above. There are
two obvious questions! Why did we introduce a number, $\lambda$; and what
notation should we use to represent this thing? Let's start with the
first question.

The $\lambda$ is there because we want to be able to take the limit as the
change in $x^a$ becomes “closer and closer to zero,” but vectors do
not come with a size. And even if they did, we need to be able to
“divide by the distance,” and we can't divide by a vector. Introducing
the $\lambda$ solves both of those problems. Of course, you might now wonder
whether the directional derivative \emph{along} $\delta^a$ depends on the
“\emph{size}” of $\delta^a$\,--\,for example, what about the directional
derivative along $2\delta^a$?\,--\,and we will come back to that.

Notice that we are taking the derivative \emph{at} $x^a$ in the
\emph{direction} of $\delta^a$. Both $x^a$ and $\delta^a$ are elements of $V$,
but when I look at eq.~\ref{eq:directional-derivative} I always
imagine “placing” the $\delta^a$ with its tail at $x^a$ and its head
pointing away from~$x^a$. 

As to the notation, there is unfortunately a profusion of such. I have
seen $\nabla_\delta f$, $\partial_\delta f$, $D_\delta f$, and
$\text{\pounds}_\delta f$. All of these mean the same thing\,--\,unless of
course they are used to mean different things. I am going to use
$\partial_\delta f$ (and I've simplified a bit by dropping the index on
$\delta$). Thus, by, say, $\partial_v f$ I mean the directional derivative of
$f$ along~$v$. (One may consider $x$ fixed, in which case we obtain
the directional derivative at a particular point, or we may allow $x$
to range over all of $V$ in which case we obtain a \emph{function} on
$V$.)

By the way, you have probably come across the “partial derivative.”
Very roughly speaking, the partial derivative is the directional
derivative “in a coordinate direction.” Suppose you were a coordinate
person. Perhaps you are working in $\setR^2$. Of course, $\setR^2$ is
a vector space, but you are not particularly concerned with that. What
you have is a function, $f(x,y)$, defined for each pair of numbers
$(x,y)$. If you were asked to take a derivative of $f$, you would
naturally ask, “in which direction? Along $x$ or along $y$?“ For
example, you might compute
\begin{equation*}
  \frac{\partial f}{\partial x} = \lim_{\delta\to 0} \frac{f(x + \delta, y) - f(x, y)}{\delta}.
\end{equation*}
That is the “partial derivative with respect to~$x$.” Similarly, you
might compute
\begin{equation*}
  \frac{\partial f}{\partial y} = \lim_{\delta\to 0} \frac{f(x, y+\delta) - f(x, y)}{\delta},
\end{equation*}
the “partial derivative with respect to~$y$.” Since your $\delta$ is a
number, you don't have to mess around with~$\lambda$.

These partial derivatives are directional derivatives. If we now
remember that $\setR^2$ is a vector space (albeit one with a natural
coordinate system) one very natural basis to choose is the set
$\{(1,0), (0,1)\}$. Those are the “unit vectors in the $x$ and $y$
directions.” Sometimes people write these basis vectors as $\hat{x}$
and $\hat{y}$. For example, $\hat{x} = (1,0)$. (Technically speaking,
we ought to write $\hat{x}^a$ but I'm going ignore the index just for
this section.)

With all this notation, the connection between the partial derivatives
and the directional derivative is:
\begin{equation*}
  \partial_{\hat{x}}f = \frac{\partial f}{\partial x} \quad\text{and}\quad 
  \partial_{\hat{y}}f = \frac{\partial f}{\partial y} 
\end{equation*}

That was a digression. However, these formula will be useful when we
come to do actual calculations, in the same way that the matrix
representations of operators are useful for doing calculations.

Let us return to the directional derivative and, for the moment,
concentrate on the directional derivative at a particular
point,~$x^a$. One way to think of the directional derivative at $x^a$
is as rule: given some vector, $\delta^a$, the directional derivative of
$f$ at $x^a$ produces a number, $(\partial_\delta f)(x^a)$. In other words, we
have a map:
\begin{align*}
  \partial \colon & V \to \setR \\
           & \delta^a \mapsto \partial_\delta f.
\end{align*}
In fact, this map is a linear map! To see this, suppose $u^a$ and
$v^a$ are vectors and consider the function $g(\alpha,\beta)$ given by
\begin{equation*}
  g(\alpha,\beta) = f(x^a + \alpha u^a + \beta v^a).
\end{equation*}
That is, $g(\alpha,\beta)$ is the function $f$, evaluated at $\alpha$ away from $x^a$
in the $u^a$ direction and $\beta$ away from $x^a$ in the $v^a$
direction. From ordinary calculus, we can write
\begin{equation*}
  g(\alpha,\beta) = g(0,0) + \alpha\frac{\partial g}{\partial \alpha} + \beta\frac{\partial g}{\partial \beta} +
  \text{terms of $O(\alpha^2)$, $O(\beta^2)$, and $O(\alpha\beta)$}.
\end{equation*}




, and consider the directional derivative along
$u^a+v^a$. We have
\begin{equation*}
\partial_{u+v}f = \lim_{\lambda\to 0}\frac{f(x^a + \lambda(u^a+v^a)) - f(x^a)}{\lambda}. 
\end{equation*}












\end{document}
