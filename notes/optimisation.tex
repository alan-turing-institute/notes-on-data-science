\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{beton}
\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{microtype}
\usepackage{booktabs}
% \usepackage[medium, compact]{titlesec}
%\usepackage[inline]{asymptote}
%\usepackage{tikz-cd}
\DeclareFontSeriesDefault[rm]{bf}{sbc}
% \usepackage{amssymb}
%% Turing grid is 21 columns (of 1cm if we are using A4)
%% Usually 4 "big columns", each of 4 text cols plus 1 gutter col;
%% plus an additional gutter on the left.
\usepackage[left=1cm, textwidth=11cm, marginparsep=1cm, marginparwidth=7cm]{geometry}
\usepackage[Ragged, size=footnote, shape=up]{sidenotesplus}
%% We used to use a two-column layout
% \setlength{\columnsep}{1cm}
\title{Increasingly tricky minimisation problems}
\author{James Geddes}
\date{\today}
%%
\DeclareBoldMathCommand{\setR}{R}
\DeclareBoldMathCommand{\bfC}{C}
\DeclareBoldMathCommand{\bfG}{\Gamma}
\newcommand{\id}{\mathbold{1}} 
\newcommand{\bzero}{\mathbold{0}} % I don't know why \bm{0} fails.
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\nullspace}{null}
\newcommand{\eg}{\emph{Example:}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\isdef}{\mathrel{\stackrel{\text{def}}{=}}}
\hyphenation{anti-sym-met-ric}
\begin{document}
\maketitle

We consider a series of increasingly tricky minimisation problems. 

\section{Problem 1: A one-dimensional quadratic} \label{sec:one-d-quadratic}
  
\emph{Problem:} Find
\sidenote*{\emph{Exercise:} Find the value of $x$ that minimises the
  function $f(x) = x^2 - 4x + 5$.}
\begin{equation}
  x_\text{min} = \argmin_{x\in\setR} f(x), \quad\text{where $f(x) = \alpha + \beta x + \gamma x^2$}.
\label{problem-quadr-one-d}
\end{equation}
\emph{Solution:} A solution exists if and only if $\gamma>0$; and, in
that case, $x_\text{min} = -\frac{1}{2}\gamma^{-1}\beta$.

Why is that the solution? Here are two ways to approach the
problem. One might “complete the square,” in the term on the right;
that is, write $f(x)$ as:
\begin{equation*}
  f(x) = \gamma\bigl(x + \frac{1}{2}\gamma^{-1}\beta\bigr)^2 - \frac{\beta^2}{4\gamma}. 
\end{equation*}
(To see that this reproduces the original expression for $f(x)$,
expand the right-hand side.) Now, the second term on the right is
constant and therefore cannot affect the location of the minimum. And
observe that the first term on the right is strictly positive if and
only if $\gamma>0$ in which its minimum value is therefore zero, and it
attains that minimum precisely when $x = \frac{1}{2}\gamma^{-1}\beta$, as
claimed above.

Another way to approach the problem is to note that, at a minimum, the
derivative of $f(x)$ must be zero. That is,
\begin{equation*}
  \left.\frac{df}{dx}\right|_{x = x_\text{min}} = 0.
\end{equation*}
In the general case, there may be other places where the derivative is
zero (namely, at a maximum or a point of inflection) and in any case
we may only have located a \emph{local} minimum, not necessarily the
true, global minimum. Nevertheless, let's set the derivative to zero
and see what falls out. We have
\begin{equation*}
  \frac{df}{dx} = \beta + 2 \gamma x,
\end{equation*}
which is zero only when $x = -\frac{1}{2}\gamma^{-1}\beta$. On this approach,
we must in addition do some extra work to decide whether we have found
a minimum (as opposed to a maximum or a point of inflection). That
extra work is to examine the sign of the second derivative,
$d^2f/dx^2$, which will be strictly positive at a minimum. Indeed,
since
\begin{equation*}
  \frac{d^2f}{dx^2} = 2\gamma,
 \end{equation*}
we conclude that we have found a minimum only when $\gamma> 0$. This is th
only minimum so it must be a global one.

\section{Problem 2: A multi-dimensional quadratic}

\emph{Problem:} Find the values of $x_1$, $x_2$, \dots, $x_n$ for
which the function $f(x_1, x_2, \dotsc, x_n)$ is
minimised.\sidenote*{\emph{Exercise}: Consider the function of two
  variables, $f(x,y) = 1 + x + y + x^2 + y^2 + xy$. For what value of
  $x$ and $y$ is $f(x,y)$ minimised?}

In general, this is a much harder problem. To make some progress we
shall make two simplifications. First, we suppose that the function
$f$ is defined not over some arbitrary $n$-dimensional space but over
an $n$-dimensional, real vector space, $V$. That will allow us to make
the second simplification, which is to suppose that $f$ has a
particularly simple form, a multi-dimensional generalisation of the
quadratic from
Section~\ref{sec:one-d-quadratic}.

We digress to introduce some notation.

\subsection{Notation for vectors (and other objects of that ilk)}

For $V$ a vector space, we write $v^a$ to represent an element of
$V$. \begin{margintable}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{@{}l@{\hspace{4pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{}}
    \toprule
    Concept & A.I. & \multicolumn{5}{c}{Alternative notations} \\
    \midrule
    Vector      & $v^a$      & $\mathbold{v}$   & $\vec{v}$
                             & $v$ & $v^i$ & $\lvert v\rangle$ \\
    Dual vector       & $w_a$      & $\mathbold{w}^T$ & $\tilde{w}$
                             & $w$ & $w_j$ & $\langle w\rvert$\\
    Operator    & $T^a{}_b$  & $\mathbold{T}$   & $\overline{T}$
                             & $T$ & $T^i{}_j$ & $T$ \\
  Contraction & $v^a w_a$  & $\mathbold{w}^T\mathbold{v}$ 
                & $\mathbold{v}\cdot\mathbold{w}$ & $w(v)$ & $v^iw_j$ & $\langle
    w \mid v \rangle$ \\   
  \bottomrule
  \end{tabular}
  \caption{Various notations for the objects in vector spaces and
    related spaces. The notation shown in the first column (labelled
    “A.I.”) is “abstract index notation.”\label{tab:notation}}
\end{margintable}

The superscript, in this case “$a$,” should be a roman letter towards
the beginning of the alphabet. Its meaning is simply that $v^a$ is a
vector; it does \emph{not} denote that $v^a$ is the $a$th component of a
vector in some basis. The expression $v^b$ is the same
vector. Similarly, $w_a$, with the index lowered, denotes an element
of the dual space, $V^*$. This notation, due to Penrose, is known as
\emph{abstract index notation}.

There are other ways of notating an element of some vector space (see
table~\ref{tab:notation} for some example). A typical approach in a
mathematics text is to write simply $v\in V$, with no adornment of the
symbol. A physicist might instead decorate the vector, as in
$\mathbold{v}\in V$, or $\vec{v}\in V$.

A problem with these other notations is that they do not naturally
extend to other denizens of the vector universe. For example, if one
frequently encounters dual vectors, then the mathematician now has to
remember which symbols are being used for dual vectors, which for
vectors, and which for ordinary numbers. A physicst, which using a
distinguishing typography for vectors, typically struggles with things
other than vectors. For example, a dual vector might be written using
the cumbersome (and coordinate-dependent) transpose,
$\mathbold{v}^T$.\sidenote*{What does $\mathbold{v}^T$ mean? It
  means, “the matrix representation of that dual vector obtained, from
  the vector $\mathbold{v}$, by means of the isomorphism between $V$
  and $V^*$ generated by some particular basis that I have in mind.”}
And of course there are other spaces whose inhabitants are useful: the
space of linear maps $V\to V$, say, or the space $V\to V^*$ (where
bilinear forms live). How should these be written?

Abstract index notation provides a convenient approach to denoting all
of these objects. Consider, for example, the dual space, $V^*$, which
is the space of linear maps from $V$ to the reals (or the
complexes). A mathematician might write, say, $w(v)$ to denote the
action of $w\in V^*$ on $v\in V$. That is straightforward, yet some things
are lost. The value $w(v)$ is, by definition, linear in both $w$ and
$v$ but that linearity is not apparent from the notation. The action
of $w$ on $v$ can equally well be thought of as the action of $v$ on
$w$ but that symmetry is not apparent either.

In abstract index notation, the action of $w_a\in V^*$ on $v^a\in V$ is
written $w_av^a$. That is, we write the $w_a$ and $v^a$ next to each
other, as if we were “multiplying” them, and ensure that the
\emph{same} index is used for both. (The expression $w_av^b$, with two
different indices, means something else.) Now the linearity and
symmetry are clearer: although this expression does \emph{not} denote
an ordinary product of numbers, it looks like one, and therefore looks
like it ought to distribute over addition, in the sense that
$w_a(u^a+v^a) = w_au^a + w_av^a$. Which it does.

We may sometimes have occasion to choose a basis for $V$. In that
case, we will denote the $\mu$th component of $v^a$ in that basis by
$v^\mu$. The distinction is that now we have written a greek index. That
is to say, whereas $v^a$ is a vector, the \emph{Ding an sich}, the
expression $v^\mu$ denotes a number, one number for each
basis. Likewise, by $w_\nu$ we will mean the $\nu$th element of the dual
vector $w_a$ with respect to some basis on $V^*$ (which will almost
always be dual basis to whatever basis we have chosen for~$V$).

The following highly convenient fact is one reason that this notation
is useful: Fix a basis for $V$ and choose, for a basis of $V^*$, the
dual basis. Then for $v^a\in V$ and $w_b\in V^*$ we have
$w_av^a = \sum_{\mu=1}^{\dim V} w_\mu v^\mu$.

\sidenote*{As a matter of history, physicists have typically written
  vectors using a basis-dependent representation. As it turns out,
  physical laws involve a lot of terms like $\sum_\mu v^\mu w_\mu$. It was
  decided that it would make life easier to drop the “$\sum$” and instead
  follow the convetion that the summation is implied, so long as some
  index is repeated, once “up” and once “down.” Abstract index
  notation has the great benefit of looking \emph{just like} the
  physicists' notation whilst being coordinate-free.}

What about other objects? It turns out that this notation handles
those nicely as well, as long as there aren't too many “individual”
vector spaces around. For example, we denote an element of
$\mathcal{L}(V,V^*)$ by means of two lowered indices, like so:
$Q_{ab}$. To act on some $v^a\in V$ we write $Q_{ab}v^b$. Note that
after we have “paired up” the $b$s, there is only one index left
“free,” and that index, $a$, is lowered. Our convention is that a
thing with a lowered index is an element of $V^*$ and that is indeed
what the action of $Q$ produces.

There's no \emph{content} to the above; it's just notation, albeit a
very suggestive one.\sidenote{Although you could make the case that
  the content of this notation is the existence of the natural
  isomorphism between $\mathcal{L}(A\otimes B, C)$ and
  $\mathcal{L}(A, \mathcal{L}(B, C))$}. It is suggestive because, if one chooses a basis
for $V$ (and the corresponding dual basis for $V^*$), then one obtains
true formulae on replacing roman superscripts and subscripts by greek
ones and adding summations where necessary. For example, suppose
$Q_{\mu\nu}$ is the matrix representation of $Q_{ab}$. Then the
$\mu$th component of $Q_{ab}v^b$ is precisely $\sum_\nu Q_{\mu\nu}v^\nu$.

Abstract index notation has some additional advantages when we come to
take gradients.

\subsection{A multi-dimensional quadratic}

Now that we have the appropriate notation we can state the problem of
this section.

\emph{Problem:} Fix a finite-dimensional, real vector space,~$V$. Let
$\alpha\in\setR$ be a number, $w_a\in V^*$ a dual vector, and
$Q_{ab} \in \mathcal{L}(V,V^*)$ a symmetric, positive-definite, bilinear
form. Find 
\begin{equation}
x^a_\text{min} = \argmin_{x^a\in V} f(x^a),\quad\text{where $f(x^a) = \alpha + w_ax^a + Q_{ab}x^ax^b$}.
\label{eq:quadr-vect}
\end{equation}

In some sense, this is the simplest, non-trivial, function we might
have attempted to minimise. We can't simplify much futher: a constant
function attains its minimum everywhere; and a linear function has no
minimum value.

Notice that even to say what we mean by a “multidimensional
quadratric” we had to use the vector space structure of $V$ (in order
to introduce $w_a$ and $Q_{ab}$).

\subsubsection{Solution by completing the square}

\emph{Solution} (to eq.~\eqref{eq:quadr-vect}):
\begin{equation}
  x^a_\text{min} = -\frac{1}{2} {(Q^{-1})}^{ab}w_b.
\label{eq:sol-quadr-vect}
\end{equation}

Before explaining why this is the solution, we describe what it
means. What is the term ${(Q^{-1})}^{ab}$? Ignoring the indices for a
minute, it is the inverse of the map $Q$. Since
$Q\in\mathcal{L}(V, V^*)$, the inverse, $Q^{-1}$, must be an element of
$\mathcal{L}(V^*, V)$ (that is, it goes the other way). In other words,
$Q^{-1}$ acts on an element of the dual space, such as $w_a$, to
produce an element of~$V$. In abstract index notation, that is
expressed by writing $Q^{-1}$ with two raised indices. To say that
$Q^{-1}$ is the inverse of $Q$ is to say that
${(Q^{-1})}^{ac}Q_{cb} = \id^a{}_b$, where, on the right, $\id^a{}_b$
is the identity operator on~$V$.

Why is this the solution? By analogy with the one-dimensional case, we
might try to “complete the square,” by rewriting $f(x^a)$ as:
\begin{equation*}
  f(x^a) = Q_{ab}\Bigl(x^a +
  \frac{1}{2}{(Q^{-1})}^{ac}w_c\Bigr)\Bigl(x^b +
  \frac{1}{2}{(Q^{-1})}^{bc}w_c\Bigr) - \frac{1}{4}w_aw_b{(Q^{-1})}^{ab}.
\end{equation*}
The right-hand side looks rather complicated. However, the two terms
in large parentheses are in fact the same vector. That is to say, if
we set
\begin{equation*}
  \xi^a = x^a +  \frac{1}{2}{(Q^{-1})}^{ac}w_c,
\end{equation*}
then $f$ becomes the much simpler-looking
\begin{equation*}
  f(x^a) = Q_{ab}\xi^a\xi^b + \text{a constant}.
\end{equation*}
Now, noting that $Q_{ab}$ is positive-definite\,--\,meaning that
$Q_{ab}\xi^a\xi^b >0$ unless $\xi^a=\bzero$\,--\,we conclude that the condition
for $x^a$ to be a minimiser is that $\xi^a = \bzero$ (the zero vector)
from which eq.~\eqref{eq:sol-quadr-vect} follows.

Alternatively, we might try taking the derivative again. For that, we
need to understand how to take derivatives of functions on a vector
space.

\subsection{Gradients}

The derivative of a function of one variable, $f(x)$, is an answer to
the question, “how fast does $f$ increase as $x$ increases?” We'd like
to ask the same question of a function on a vector space, $f(x^a)$,
where now $x^a\in V$. (Remember that the “$a$” in $x^a$ merely means
that $x^a$ is a vector.)

In contrast to the one-dimensional case, however, the rate of change
of $f$ at $x^a$ will, in general, now depend on the \emph{direction}
in which one is “headed away from~$x^a$,” and so any straightforward
extension of the one-dimensional derivative will need to specify such
a direction.  Fortunately, we have a convenient object with which to
capture the idea of a direction, and that is a vector.

Thus, let $\delta^a$ be a vector. Instead of asking, ``what is the rate of
change of $f(x)$ as $x$ increases?'' we shall ask, ``what is the rate
of change of $f(x^a)$ as $x^a$ changes in the direction
of~$\delta^a$?''\sidenote{Both $x^a$ and $\delta^a$ are elements of
  $V$ but, while we are using $x^a$ to denote a point in $V$, we are
  using $\delta^a$ to denote a displacement from $x^a$. I always imagine
  the $\delta^a$ as having its ``tail'' at $x^a$ and its ``head'' pointing
  away from~$x^a$.} And, in order to capture the idea of ``moving away
from $x^a$ along $\delta^a$,'' we shall introduce a number, $\lambda$, and
consider the point $x^a+\lambda\delta^a$ as $\lambda$ changes. This point has the
property that it is $x^a$ when $\lambda=0$, and ``moves in the direction of
$\delta^a$'' as $\lambda$ increases.

These remarks lead to the following definition.

\emph{Definition:} The \textbf{directional derivative of $f$ along
  $\delta^a$}, which we write as $\partial_\delta f$, is defined by\sidenote*{There is
  unfortunately a profusion of notation for the directional
  derivative. As well as $\partial_\delta f$, I have seen
  $\nabla_\delta f$, $D_\delta f$, and $\text{\pounds}_\delta f$. All of these mean the
  same thing\,--\,unless of course they are used to mean different
  things.}
\begin{equation}
\partial_\delta f \isdef \frac{d}{d\lambda} f(x^a+\lambda\delta^a) \quad\text{evaluated at $\lambda = 0$}.
  \label{eq:directional-derivative}
\end{equation}
\sidenote*{\emph{Exericse:} Show that in one dimension
  eq.~\eqref{eq:directional-derivative} reduces to the usual
  derivative, $df/dx$, under the choice $\delta=1$. Hint: Use a change of
  variables, $u = x+\lambda$.}

The directional derivative of a function $f$ depends on the
direction,~$\delta^a$. Perhaps surprisingly, however, it turns out that
after all it is possible to capture a notion of ``the'' derivative of
$f$, independent of the direction. The idea is to think of
$\partial_\delta f$ as a function of $\delta^a$: it is this function that is ``the''
derivative. In other words, ``the'' derivative of $f$ at $x^a$ is that
map $\phi\colon \delta^a\to\setR$ defined by
$\phi(\delta^a) = \partial_\delta(f)$ (evaluated at $x^a$).

On the face of it, one might imagine that this $\phi$ is some terribly
complicated thing, depending in various difficult-to-express ways on
its argument,~$\delta^a$. Remarkably, however, it is not; it is in fact, a
linear map. In other words, $\phi$ (as a function) is an element of the
dual space~$V^*$.

To see this, let $u^a$ and $v^a$ be vectors. We shall show that
$\partial_{\alpha u+\beta v}f = \alpha\partial_u f+ \beta\partial_v f$. Let
$\tilde{\alpha}(\lambda)$ and $\tilde{\beta}(\lambda)$ be functions of
$\lambda$ and consider the derivative of the function
$f(x^a+\tilde{alpha}u^a+\tilde{\beta}v^a)$,
\begin{equation*}
  \frac{df}{d\lambda} = \frac{\partial f}{\partial \tilde{\alpha}} \frac{d\tilde{\alpha}}{d\lambda} +
  \frac{\partial f}{\partial \tilde{\beta}} \frac{d\tilde{\beta}}{d\lambda}
  \label{eq:chain-rule}
  \end{equation*}
  (using the chain rule). Now, consider the special case of
  $\tilde{\alpha}=\alpha\lambda$ and $\tilde{\beta}=\beta\lambda$. On the left-hand side of
  eq.~\eqref{eq:chain-rule} we have:
  \begin{equation*}
    \frac{df}{d\lambda} = \frac{d}{d\lambda} f\bigl(x^a+\lambda(\alpha u^a +\beta v^a)\bigr), 
  \end{equation*}
which, evaluated at $\lambda=0$, is the directional derivative $\partial_{\alpha u+\beta v}
f$.

To evaluate the right-hand side of eq.~\eqref{eq:chain-rule}, note
that $d\tilde{\alpha}/d\lambda = \alpha$ and
$d\tilde{\beta}d\lambda = \beta$. Furthermore,
$\partial f/\partial \tilde{\alpha}$, evaluated at
$\tilde{\alpha}=\tilde{\beta}=0$ is precisely the directional
derivative~$\partial_u f$. Thus we have,
\begin{equation*}
  \partial_{\alpha u+\beta v} f = \alpha\partial_u f + \beta\partial_v f.
\end{equation*}
In other words, the map $\partial$ is a linear map.











By the way, you have probably come across the “partial derivative.”
Very roughly speaking, the partial derivative is the directional
derivative “in a coordinate direction.” Suppose you were a coordinate
person. Perhaps you are working in $\setR^2$. Of course, $\setR^2$ is
a vector space, but you are not particularly concerned with that. What
you have is a function, $f(x,y)$, defined for each pair of numbers
$(x,y)$. If you were asked to take a derivative of $f$, you would
naturally ask, “in which direction? Along $x$ or along $y$?“ For
example, you might compute
\begin{equation*}
  \frac{\partial f}{\partial x} = \lim_{\delta\to 0} \frac{f(x + \delta, y) - f(x, y)}{\delta}.
\end{equation*}
That is the “partial derivative with respect to~$x$.” Similarly, you
might compute
\begin{equation*}
  \frac{\partial f}{\partial y} = \lim_{\delta\to 0} \frac{f(x, y+\delta) - f(x, y)}{\delta},
\end{equation*}
the “partial derivative with respect to~$y$.” Since your $\delta$ is a
number, you don't have to mess around with~$\lambda$.

These partial derivatives are directional derivatives. If we now
remember that $\setR^2$ is a vector space (albeit one with a natural
coordinate system) one very natural basis to choose is the set
$\{(1,0), (0,1)\}$. Those are the “unit vectors in the $x$ and $y$
directions.” Sometimes people write these basis vectors as $\hat{x}$
and $\hat{y}$. For example, $\hat{x} = (1,0)$. (Technically speaking,
we ought to write $\hat{x}^a$ but I'm going ignore the index just for
this section.)

With all this notation, the connection between the partial derivatives
and the directional derivative is:
\begin{equation*}
  \partial_{\hat{x}}f = \frac{\partial f}{\partial x} \quad\text{and}\quad 
  \partial_{\hat{y}}f = \frac{\partial f}{\partial y} 
\end{equation*}

That was a digression. However, these formula will be useful when we
come to do actual calculations, in the same way that the matrix
representations of operators are useful for doing calculations.












\end{document}
