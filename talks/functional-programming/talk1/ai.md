Okay, that's helpful.

I guess I think that (1) is true, but only in the trivial sense that it's _physically possible_ to make such a thing. But I don't think we're even close to knowing what such a thing would look like.

If people want to worry about possible risks -- or the singularity, or whatever -- that's of course all fine, but I don't think we're anywhere close to it being a pressing concern. Some people worry about the risks of emitting radio signals into space, because it'll tell the aliens we're here. I mean -- it's not against the laws of physics, so maybe? But I'd think that asteroid impacts, say, are more worrisome.

The field has made some pretty cool advances in the last 50 years. 



Here's my anecdotal take on history:

People have worried about the "AI goes rogue and takes over" scenario forever. Just remembering things that I've seen or read, there was a short-short story "Answer" by Frederic Brown in 1954 (back when we called it "cybernetics", not that I was alive *then*); and a movie, "Collosus: The Forbin Project" from 1970, about the time of the first wave of AI.

Of course there *was* no AI back in 1954 and so not a lot of point in elevating the risk to the same level as, eg, nuclear war. By 1970, SHRDLU could perform what looked like simple reasoning tasks, but it was clear that what it was actually doing was tree search over a limited set of moves: and since we didn't (and don't) know how to represent "go rogue and take over the world" as a sequence of such moves, it was unlikely to be a threat.

That didn't stop the researchers confidently predicting that "we'd have vision solved in a summer, and machine translation in a couple of years." In 1970 mind you!

In 19 Deep Blue was upgraded from the capacity to search 1million moves per second to 10 million moves per second, and XYZ commented that "now it plays like a human". Back then we also had systems that could solve maths problems (like integration) symbolically and even *prove* logical assertions in what looked very much like reasoning.

Researchers confidentally predicted that self-driving cars were just around the corner and that the Japanese 5th generating computing project would leapfrog the US by a decade. [LINK TO THE DARK AGES OF AI PAPER]

People also made a lot of claims about having cracked (one or other aspect) of cognition: Cyc was a repository of "common-sense knowledge"; some people proposed systems that could "reason by analogy". 

The thing is, the results were impressive! I mean, proving assertions in logic is hard! It seemed casuistic to argue that these machines weren't _really_ intelligent, just because one couldn't see intelligence _inside_ them. That's just moving the goalposts: Surely intelligence is as intelligence does? Just look at the results!





So one question is, is the kind of AI that we have now


; and even in 1970 the most you could do was move a few blocks around.



What seems to be happening now is that people have decided that large language models are "sufficiently close" to AGI, that 


There's a sort of historical pattern here. 




Someone once said something like, "If an old scientist tells you something can't be
done, you can ignore them; but if they tell you it can be done, they
are probably right." I forget the exact quote. I am a greybeard (not
entirely sure how that happened, but here we are) so you may wish to
ignore 
