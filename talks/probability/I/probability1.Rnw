\documentclass[11pt]{article}
%\usepackage{graphicx}
%\usepackage{textcomp}
%\usepackage{amssymb}
%\usepackage{fontspec}
%\usepackage{minted}
\usepackage[T1]{fontenc}
\usepackage{concrete}
\usepackage{euler}
\usepackage[paperwidth=209.6mm, paperheight=157.2mm,
  right=0.1875in, top=0.1875in, bottom=0.1875in, footskip=0.164in, textwidth=85mm,
  marginparwidth=40mm, marginparsep=10mm]{geometry}
\usepackage{layout}
\usepackage{parskip}
\usepackage{tabularx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{fancyhdr}
%%
\usepackage[style=authoryear]{biblatex}
\addbibresource{probability.bib}
%%
\usepackage{amsmath}
%%
\author{James Geddes}
\date{\today}
\title{Probability I}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[L]{\scriptsize\thepage}
%%
\begin{document}
%\SweaveOpts{concordance=TRUE}
\setcounter{page}{0}
\newgeometry{textwidth=200mm}
\maketitle\thispagestyle{empty}
\restoregeometry
\newpage
%% ============================================================================
%% Setup
%%
<<R_setup, include=FALSE>>=
library(tidyverse)
library(ggthemes)
options(width = 46, scipen = -2, digits = 2)
<<R_convenience_functions>>
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,
                      dev='tikz', fig.width=3.3, fig.height=2,
                      # prompt=TRUE, comment='',
                      tidy=TRUE, highlight=FALSE, tidy.opts=list(width.cutoff=I(46)))
set.seed(20210807)
@ 
%% ============================================================================
%% Slide: Ensembles
%%
\label{slide:ensemble}
<<initialisation>>=
jones1 <- read_tsv("Jones2004_Single.txt", quote = "") %>%
  filter(str_detect(Char, "[:lower:]|[:space:]"))

nyt <- jones1 %>% 
  select(letter = Char, NYT) %>%
  mutate(letter = factor(letter, levels = letters, ordered = TRUE), frequency = NYT/sum(NYT))

jones2 <- read_tsv("Jones2004_Bigram.txt", quote="") %>%
    filter(str_detect(Pred, "[:lower:]|[:space:]")) %>%
    filter(str_detect(Succ, "[:lower:]|[:space:]"))

## If any count is zero, make it 1 to avoid "impossible" likelihoods.
nyt2 <- jones2 %>%
    select(first = Pred, second = Succ, NYT) %>%
    mutate(NYT = ifelse(NYT == 0, 1, NYT)) %>%
    mutate(first = factor(first, levels = letters, ordered = TRUE),
           second = factor(second, levels = letters, ordered = TRUE),
           frequency = NYT/sum(NYT))
@ 

\begin{center}
<<letter_frequency_plot, cache=TRUE>>=
ggplot(data = nyt) +
    geom_col(aes(letter, frequency)) +
    theme_tufte() +
    theme(axis.title = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.x = element_text(margin = margin(t = -6)))
@
\end{center}
Frequencies of individual lower-case letters in a sample of the New York
Times. The sample space is $\mathcal{A} = \{\text{a},
\text{b}, \dotsc, \text{z}\}$ and the probability mass function is $p(\text{a})
= \Sexpr{nyt$frequency[nyt$letter == "a"]}$, etc. From \textcite{jones2004}.
\medskip

\begin{center}
<<cumulative_letter_frequency_plot>>=
nyt <- nyt %>% mutate(cumulative = cumsum(frequency))
r <- runif(1)
l <- with(nyt, 
      if (r <= cumulative[1]) {
        letter[1]
      } else {
        letter[-1][cumulative[-length(cumulative)] < r & r <= cumulative[-1]]
      })
ggplot(data = nyt) + 
  geom_segment(aes(x = "a", y = r, xend = l, yend = r), colour = "grey", linetype = "dashed") +
  geom_segment(aes(x = l, y = 0, xend = l, yend = r), colour = "grey", linetype = "dashed") +
  geom_text(aes(x = "a", y = r, label = format(r, digits = 3)), hjust = "left", vjust = "bottom", nudge_y = 0.02) +
  geom_step(aes(x = letter, y = cumulative, group = 1)) +
  theme_tufte() + theme(axis.title = element_blank())
@
\end{center}
How to randomly select the letter `\Sexpr{l}.'
\newpage
%%
%% ============================================================================
%% Slide: Entropy (1)
%%
\label{slide:entropy}%

\hrulefill

<<monogram_sample, size="small", comment=''>>=
monogram_sample <- sample(1:26, size = 100, prob = nyt$frequency,
                          replace = TRUE)
cat(letters[monogram_sample], sep = ", ", fill = 47)
@
A sample of 100 characters from the `English monogram' ensemble. The average information content
of the outcomes in this sample is
$\Sexpr{sum(log2(1/nyt$frequency[monogram_sample]))/length(monogram_sample)}$
bits per letter.
\medskip

\hrulefill

<<uniform_letter_sample, size = "small", comment=''>>=
letter_sample <- sample.int(26, 100, replace = TRUE)
cat(letters[letter_sample], sep=", ", fill = 47)
@
A set of letters sampled \emph{uniformly} from the alphabet. The entropy of the
uniform distribution on the alphabet is $H = \log_2 26 =
\Sexpr{log2(26)}$ bits. However, with respect to the English monogram ensemble,
the average information content of the outcomes in this sample 
is $\Sexpr{sum(log2(1/nyt$frequency[letter_sample]))/length(letter_sample)}$ bits per letter.

\newpage
\label{slide:challenge}
%% ============================================================================
%% Decryption challenge (1)
%% 
<<cipher_definition>>=
ciphertext <- encode("btjpxrmlxpcuvamlxicvjpibtwxvrcimlmtrpmtnmtnyvcjxcdxvmwmbtrjjpxamtngxrjbahuqctjpxqgmrjxvcijpxymggcijpxhbtwrqmgmaxmtnjpxhbtwrmyjpxqmvjcijpxpmtnjpmjyvcjxjpxtjpxhbtwracutjxtmtaxymrapmtwxnmtnpbrjpcuwpjrjvcufgxnpblrcjpmjjpxscbtjrcipbrgcbtryxvxgccrxnmtnpbrhtxxrrlcjxctxmwmbtrjmtcjpxvjpxhbtwavbxnmgcunjcfvbtwbtjpxmrjvcgcwxvrjpxapmgnxmtrmtnjpxrccjprmexvrmtnjpxhbtwrqmhxmtnrmbnjcjpxybrxlxtcifmfegctypcrcxdxvrpmggvxmnjpbryvbjbtwmtnrpcylxjpxbtjxvqvxjmjbctjpxvxcirpmggfxagcjpxnybjpram")
@

<<chiphertext, comment=''>>=
cat(as.character(ciphertext), fill = 43, sep = "")
@
The first of the `cipher challenges' accompanying
\emph{The Code Book} \autocite{thecodebook}. It has been encrypted using a
monoalphabetic substition cipher.

\newpage
\label{slide:attempt:one}
%% ============================================================================
%%
%% Attempting to crack the code
%%
<<english_order, size="small", echo = 1>>=
english_common <- order(nyt$frequency, decreasing = TRUE)
cat(letters[english_common], sep = "")
@


<<cipher_order, size="small", echo = 1>>=
cipher_common <- order(tabulate(ciphertext, nbins=26), decreasing = TRUE)
cat(letters[cipher_common], sep = "")
@ 
The most commonly-occuring letters in English and in the ciphertext.

\hrulefill\medskip

<<pi_v1, size="small">>=
pi_ml <- cipher_common %after% perm_inverse(english_common) 
cat(letters, "\n",
#    rep("| ", times = 13), "\n",
    letters[pi_ml], sep="")
@ 

<<ml_plaintext1, size="small", comment=''>>=
plaintext1 <- decipher(pi_ml, ciphertext)
cat(letters[plaintext1], fill = 46, sep = "")
@
The maximum-likelihood permutation and the corresponding plaintext.

\newpage
%% ============================================================================
%% Joint probabilities
%%
\label{slide:joint}%
<<bigrams_plot, fig.width=3.5, fig.height=3.5, cache=TRUE>>=
ggplot(data = nyt2) +
    geom_point(aes(second, first, size = frequency), shape = "square", stroke = 0) +
    scale_size_area(max_size = 4) + scale_x_discrete(position = "top") +
    scale_y_discrete(limits = rev) + theme_tufte() + theme(legend.position =
                                                               "none",
                                                           axis.ticks =
                                                               element_blank())
@

The joint ensemble over \emph{bigrams:} pairs of consecutive lowercase letters
in English. The sample space is $\mathcal{A}\times \mathcal{A}$. The marginal
probabilities are the single-letter frequencies from before.


\newpage
%% ============================================================================
%% A markov chain model
%%
\label{slide:model2}%

<<bigram_sample, size="small", comment=''>>=
N <- 100
bigram_sample <- rep(0, 100)
bigram <- matrix(nyt2$frequency, nrow = 26, dimnames = list(letters, letters))
init <- sample.int(26 * 26, size = 1, prob = bigram)
bigram_sample[c(1,2)] <- c(nyt2$first[init], nyt2$second[init])
for (i in 3:N) {
    bigram_sample[i] <- sample.int(26, size = 1, prob = bigram[, bigram_sample[i-1]])
}
bsl <- function(n) { letters[bigram_sample[n]] } # Convenience function 
cat(letters[bigram_sample], sep = "", fill = 46)
@
A sequence of 100 letters sampled from an ensemble on $\mathcal{A}^{100}$
having the correct bigram distribution.
\medskip

\begin{center}
\begin{tabularx}{\textwidth}{@{}llc@{}>{\raggedleft\arraybackslash}X@{}}
  \toprule
  \multicolumn{2}{@{}l}{Distribution} & Example & Entropy (bits) \\
  \midrule
  Monogram & $P(l)$          & `u' & $\Sexpr{with(nyt, sum(frequency *
    log2(1/frequency)))}$  \\
  Conditional & $P(l_2 \mid l_1)$ & `u' (after `q') & $\Sexpr{with(nyt2, sum(frequency * log2(1/frequency))) - with(nyt, sum(frequency * log2(1/frequency)))}$ \\
  Product & $P(l_1) P(l_2)$   & `q', `u' & $\Sexpr{2 * with(nyt, sum(frequency *
    log2(1/frequency)))}$  \\
  Bigram & $P(l_1 l_2)$       & `qu' & $\Sexpr{with(nyt2, sum(frequency *
  log2(1/frequency)))}$ \\
  \bottomrule
\end{tabularx}
\end{center}
The entropy of four related ensembles (two on $\mathcal{A}$ and two on $\mathcal{A}\times\mathcal{A}$).\bigskip

\begin{center}
  \begin{tabular}{@{}lr@{}}
  \toprule
  Text                    & \parbox{1.5in}{\raggedleft{}Information content
    (bits / letter)} \\
  \midrule 
  Ciphertext      & $\Sexpr{-bigram_log_likelihood(ciphertext, bigram)/length(ciphertext)}$ \\
  `Decrypt' (first attempt) & $\Sexpr{-bigram_log_likelihood(plaintext1,
    bigram)/length(ciphertext)}$\\
  Expected plaintext & $\Sexpr{with(nyt2, sum(frequency * log2(1/frequency))) - with(nyt, sum(frequency
  * log2(1/frequency)))}$\\                   
  \bottomrule
\end{tabular}
\end{center}
The information content of three sequences of letters of letters with respect to
the bigram distribution.


%% ============================================================================
%% Attempt 2 on the ciphertext
%%
\newpage
\label{slide:attempt2}

\begin{center}\small
\begin{tabular}{@{}rlr@{}}\toprule
  Step & Key & $h_b/\text{letter}$ \\ \midrule
  <<dicipher_two, results="asis">>=
  marginal <- log2(apply(bigram, MARGIN = 2, sum))
  normalisation <- apply(bigram, MARGIN = 1, sum)
  conditional <- log2(bigram / rep(normalisation, each = length(marginal)))
  surprise <- function(perm) {
      -bigram_ll_fast(decipher(perm, ciphertext), marginal, conditional)
  }                     

  perm <- pi_ml
  step <- 1
  
  next_perm <- minimise_ll_step(perm, surprise)
  
  while (!isFALSE(next_perm)) {
      cat(paste0(step, " & ", perm_as_string(perm), " & ", format(round(surprise(perm)/length(ciphertext), 2), nsmall=2), "\\\\", collapse=""))
      perm <- next_perm
      step <- step + 1
      next_perm <- minimise_ll_step(perm, surprise)
  }
  @
  \bottomrule
\end{tabular}
\end{center}
\smallskip

<<ml_plaintext2, size="small", comment=''>>=
plaintext2 <- decipher(perm, ciphertext)
cat(letters[plaintext2], fill = 46, sep = "")
@
`Steepest ascent' solution to maximising the likelihood of the key. Each
step applies a transposition to the current proposed key. $h_b$ is the
information content of the resulting plaintext, given the key. 

\newpage
%% ============================================================================
%% Bayes' theorem
%%
\label{slide:bayes}
<<hat>>=
maxN <- 20
Ns <- 1:maxN
pNs <- rep.int(1/maxN, maxN) # Prior probability of N
px_given_N <- function(x, n) { ifelse(x > n, 0, 1/n) } # p(x|N), vectorised

## Joint p(x, N)
px_Ns <- outer(1:maxN, 1:maxN, px_given_N)
pxNs <- t(t(px_Ns) * pNs)
pxs <- apply(pxNs, MARGIN=1, sum) 
pN_xs <- pxNs / pxs
@

<<hat_plot_joint, fig.width=2.5, fig.height=2.5, cache=TRUE>>=
ggplot(data = as.data.frame.table(pxNs) %>%
           transmute(x = as.integer(Var1), N = as.integer(Var2), p = Freq)) +
    geom_point(aes(N, x, size = p), shape = "square", stroke = 0) +
    scale_size_area(max_size = 4) + coord_fixed() + 
    scale_x_continuous(position="top", breaks=seq(2, 20, 2)) +
    scale_y_reverse(breaks=seq(2, 20, 2)) + labs(x="$N$", y="$x$") +
    theme_tufte() + theme(legend.position = "none")
@

The joint probability $p(x, N)$ for the hat problem.

<<hat_plot_cond, fig.width=2.5, fig.height=2.5, cache=TRUE>>=
ggplot(data = as.data.frame.table(pN_xs) %>%
           transmute(x = as.integer(Var1), N = as.integer(Var2), p = Freq)) +
    geom_point(aes(N, x, size = p), shape = "square", stroke = 0) +
    scale_size_area(max_size = 4) + coord_fixed() +
    scale_x_continuous(position="top", breaks=seq(2,20,2)) +
    scale_y_reverse(breaks=seq(2,20,2)) + labs(x="$N$", y="$x$") +
    theme_tufte() + theme(legend.position = "none")
@ 

The conditional (posterior) probability $p(x\mid N)$.

\newgeometry{left=0.5in, top=0.25in, bottom=0.25in, right=0.25in}
\newpage
\label{slide:measure}
\section*{Introduction to Part II}






\newpage
\printbibliography

%% ============================================================================
%% 
%% Notes to the slides
%%
%% ============================================================================
%%
\newgeometry{left=0.5in, top=0.25in, bottom=0.25in, textwidth=30em, marginparwidth=15em}
\newpage
\section*{Notes}
\subsection*{Ensembles (slide~\pageref{slide:ensemble})}
An \emph{ensemble}, $X$, is a pair, $X = (\mathcal{A}_X, p_X)$, where
\begin{enumerate}
\item $\mathcal{A}_X$ is a set, called the \emph{sample space}, whose elements are
  called \emph{outcomes}; and
\item For each outcome $a \in \mathcal{A}_X$, $p_X(a)$ is a number in $[0,1]$ such
  that
  \[
  \sum_{a \in \mathcal{A}_X} p_X(a) = 1.
  \]
\end{enumerate}
The probability that a sample, $x$, from an ensemble, $X$, is outcome
$\text{a}$ (say) is $P_X(X = \text{a}) = p_X(\text{a})$. $p_X$ is called the
\emph{probability mass function}. Instead of $P_X(X = \text{a})$ we sometimes
write $P(\text{a})$. (This is confusing.)\marginpar{$P(\text{vowel}) =
  \Sexpr{sum(nyt$frequency[match(c("a", "e", "i", "o", "u"), letters)])}$.}

How do you sample from an ensemble? If you are lucky enough to have a source of
samples from the uniform distribution on $[0, 1]$, use the inverse cumulative
distribution.

What if you only have a random stream of 1s and 0s? Well, a real number in
$[0,1]$ is an infinite stream (eg, 0.10011010\ldots) but you only need a finite
part of the expansion to know which `step' you will eventually end up in.

The notion of an ensemble is taken from \textcite{mackay2003information}.

\newpage
\subsection*{Entropy (slide~\pageref{slide:entropy})}
The \emph{Shannon information content} of an outcome $x$ from some ensemble $X$
is\marginpar{$h(\Sexpr{l}) = \Sexpr{log2(1/nyt$frequency[nyt$letter == l])}\,\text{bits}$.}
\[
h(x) = \log_2\frac{1}{p_X(x)}.
\]

The \emph{Shannon entropy}, $H$, of an ensemble, $X$, is\marginpar{$H(\mathcal{A}) = \Sexpr{with(nyt, sum(frequency * log2(1/frequency)))} \,\text{bits}.$}
\[
H(X) = \sum_{x\in\mathcal{A}_X} p_X(x)\log_2\frac{1}{p_X(x)}.
\]

Suppose $P=(\mathcal{A}, p)$ and $Q=(\mathcal{A}, q)$ are two ensembles over the
same sample space. The \emph{Kullback--Leibler divergence} of $P$ from $Q$ is
\[
D_{\text{KL}}(P \parallel Q) = \sum_{a\in\mathcal{A}} p(a)\log_2\frac{p(a)}{q(a)}.
\]
It is the difference between the expected information
content of a sample from $P$ if the information content were measured with $q$
and the entropy of $P$:
\[
D_{\text{KL}}(P \parallel Q) = \sum_{a\in\mathcal{A}} p(a)\log_2\frac{1}{q(a)} - \sum_{a\in\mathcal{A}} p(a)\log_2\frac{1}{p(a)}.
\]
The definition applies only if $q(a) = 0 \implies p(a) = 0$. 

Theorem: $D_{\text{KL}}(P\parallel Q) \geq 0$ with equality if and only if $P=Q$. 

\newpage
\subsection*{A model of text (slide~\pageref{slide:challenge})}
To encipher plaintext $l_1 l_2 l_3 \dotsm$ (where $l_i\in\mathcal{A}$) using a \emph{monoalphabetic substitution cipher}:
\begin{enumerate}
\item Choose a permutation $\pi:\mathcal{A}\to\mathcal{A}$. (This is the key.)
\item Emit the ciphertext $\pi(l_1) \pi(l_2) \pi(l_3) \dotsm$.
\end{enumerate} 
Since there are $26! \approx \Sexpr{factorial(26)}$ keys, this cipher was thought to
be difficult to break.

Suppose the plaintext, $l_1 l_2 l_3\dotsm l_N\in\mathcal{A}^N$ were generated by
sampling $N$ letters independently from $(\mathcal{A}, p)$. (Clearly it isn't,
but hold that thought. By the way, we're imagining that $N$ is fixed.)

The probability of the plaintext under this model is
\[
P_{\mathcal{A}^N}(l_1 l_2 l_3\dotsm l_N) = p(l_1)p(l_2)p(l_3)\dotsm p(l_N).
\]

Alternatively, suppose we have ciphertext $c_1 c_2 c_3\dotsm$ and we happen to
know that the permutation used to encipher was $\pi$. Then the probability of this
cipher text (given the permutation) is
\[
P(c_1 c_2 c_3\dotsm c_N \mid \pi) = p\bigl(\pi^{-1}(c_1)\bigr) \dotsm
              p\bigl(\pi^{-1}(c_N)\bigr).
\]
The term on the left means `the probability of emitting the ciphertext
\emph{given} that the permutation is $\pi$.' Sometimes this is called the
\emph{likelihood of $\pi$ given the data}.

Notice that the likelihood, thought of as a function of $\pi$, is \emph{not} a
probability mass function. It \emph{is} a function from the space of
permutations to $[0, 1]$ but for example it is not normalised to~1. 

Still we might hope that a good guess for $\pi$ is the one that maximises the
likelihood of~$\pi$. 

\newpage
\subsection*{Attempt 1 (slide~\pageref{slide:attempt:one})}

We're attempting to find the permutation $\pi$ that maximises the likelihood: 
\[
P(c_1 c_2 c_3\dotsm c_N \mid \pi) = p(l_1) p(l_2) \dotsm p(l_N).
\]
Alternatively, we're seeking the $\pi$ for which the Shannon information content
of the computed plaintext is smallest.\footnote{It's usually easier to work with
the information content than the probabilities, as the probabilities are
tiny. Since the formula is $h = -\log_2 p$ and we're interested in the
likelihood of $\pi$, people often talk about minimising the `negative
log-likelihood.'}

In fact, we can immediately write down the answer `by inspection.' The product
probability is maximised when the letters which occur most often are the ones
with the highest probabilities. So we count the letters in the ciphertext and
rank them from most to least frequently occuring, then match up the plaintext
letters from most to least frequent.

Well, that didn't work. It doesn't even look like English. (Although it does
look like a sample from the single-letter ensemble.)

It's true that the sequence `toe' occurs very frequently---21 times---so perhaps
that sequence of three letters should really be `the'. If so, we have at least
got the two most common letters correct.

A major deficit of this approach is that we have completely ignored the
\emph{order} of letters in the supposed plaintext. The probability we are
maximising is simply the product of the individual letter probabilities: we
would have got the same answer if the ciphertext had been scrambled. Obviously,
English is not like that: some sequences of English letters are much more likely
than others.

Suppose $\mathcal{A}$ and $\mathcal{B}$ are two sets. The \emph{Cartesian
product}, $\mathcal{A}\times \mathcal{B}$ is the set of all ordered pairs $(a, b)$
where $a\in\mathcal{A}$ and $b\in\mathcal{B}$:
\[
\mathcal{A}\times \mathcal{B} \equiv \{(a, b)\mid a\in\mathcal{A} \wedge b\in\mathcal{B}\}.
\]

A \emph{sequence} (of length $N$) is an element of the Cartesian product of some
set, $\mathcal{A}$, with itself, $N$ times: $\mathcal{A}\times\mathcal{A}\overset{N}{\times\dotsb\times}\mathcal{A}$.

So what we want is a probability distribution on sequences of \Sexpr{length(ciphertext)} characters
that more closely resembles real English. Then we'll have a better shot of
finding the correct $\pi$. 



\newpage
\subsection*{Joint probability (slide~\pageref{slide:joint})}

Let $X$ and $Y$ be two independent ensembles. The \emph{product ensemble}
consists of:
\begin{enumerate}
 \item The sample space of all pairs $(x, y)$ where $x\in \mathcal{A}_X$ and $y \in
   \mathcal{A}_Y$;
 \item The probability mass function $p_{X\times Y}(x, y) = p_X(x)p_Y(y)$.
\end{enumerate}
The entropy of the product ensemble is the sum of the entropies of the individual
ensembles:
\[
\begin{aligned}
H(X\times Y) &= \sum_{x\in\mathcal{A}_X}\sum_{y\in\mathcal{A}_Y} p_X(x)p_Y(y)
\log_2\frac{1}{p_X(x)p_Y(y)} \\
&= \sum_{x\in\mathcal{A}_X}\sum_{y\in\mathcal{A}_Y} p_X(x)p_Y(y)
\Bigl(\log_2\frac{1}{p_X(x)} + \log_2\frac{1}{p_Y(y)} \Bigr) \\
&=H(X) + H(Y).
\end{aligned}
\]

Not every ensemble on a product space is a product of ensembles.

Suppose $\mathcal{A}_X$ and $\mathcal{A}_Y$ are sets. A \emph{joint ensemble}
$(\mathcal{A}_X\times \mathcal{A}_Y, p_{X\times Y})$ is
\begin{enumerate}
\item The set, $\mathcal{A}_X\times \mathcal{A}_Y$, which consists of all pairs
  $(x,y)$ of an element of $\mathcal{A}_X$ and an element of $\,mathcal_A{Y}$;
\item A map $p_{X\times Y} : X\times Y \to [0,1]$ such that
  \[
  \sum_{x\in \mathcal{A}_X}\sum_{y\in \mathcal{A}_Y} p(x,y) = 1.
  \]
\end{enumerate}
The probability of obtaining outcomes $a\in \mathcal{A}_X$ and $b\in \mathcal{A}_Y$
is $P_{X\times Y}(x = a, y = b)$. Note that there are many ensembles over $X\times Y$ and
often people don't bother to distinguish the $P$s.\marginpar{$P(\text{qu}) =
\Sexpr{with(nyt2, frequency[second == "u" & first == "q"])}$.}

\subsubsection*{Marginal probability}
The \emph{marginal probability on $\mathcal{A}_X$} is the probability of an outcome from
$\mathcal{A}_X$, ignoring what happened in $\mathcal{A}_Y$:
\[
P(x = a) = \sum_{b\in \mathcal{A}_Y}P(x = a, y = b).
\]
People sometimes write
\[
P(x) = \sum_y P(x, y).
\]

\subsubsection*{Conditional probability}
The \emph{conditional probability on $\mathcal{A}_X$ given $y$} is the probability of
an outcome $a$ from $\mathcal{A}_X$ given that the outcome from $\mathcal{A}_Y$ was
$b$. It is obtained by holding $y$ fixed and then rescaling so the total
probability is still~1:\marginpar{$P(\text{u}\mid \text{q}) = \Sexpr{with(nyt2, frequency[second == "u" &
    first == "q"]/sum(frequency[first == "q"]))}$.}
\[
P(x = a \mid y = b) = \frac{P(x = a, y = b)}{\sum_{a\in \mathcal{A}_X} P(x = a, y = b)}.
\]
People sometimes write
\[
P(x \mid y) = \frac{P(x, y)}{P(y)}.
\]

The conditional probability $P(x \mid y$) is a probability on $\mathcal{A}_X$ but it is
\emph{not} a probability on $\mathcal{A}_Y$.

\newpage
\subsection*{A new model (slide~\pageref{slide:model2})}

Can we think of a generative model for plaintext which reproduces the correct
pairwise distribution of letters?

The \emph{chain rule} is just the law of conditional probability rewritten:
\[
P(x, y) = P(y\mid x) P(x).
\]
Instead of sampling from the bigram distribution, $P(x,y)$, we can equivalently
sample the first letter from the marginal, $P(x)$, and then the next letter from the conditional
given the first letter, $P(y\mid x)$.

Continuing the chain rule, the following holds generally:
\[
P(x, y, z) = P(z\mid x, y) P(y\mid x) P(x).
\]
One way\marginpar{$h_b(\text{\Sexpr{paste0(bsl(1:4), collapse = '')}...\Sexpr{paste0(bsl(99:100), collapse = '')}}) =
  \Sexpr{-bigram_log_likelihood(bigram_sample, bigram)}\,\text{bits}$.} to
sample from a distribution with the correct bigram probabilities is to assume
that each letter is independent of all previous letters except the preceding
one. So, for $N$ letters, we would choose a sequence $l_1 l_2 \dotsm l_N$ with
probability:
\[
P_b(l_1l_2l_3\dotsm l_N) = P(l_1) P(l_2 \mid l_1) P(l_3 \mid l_2) \dotsm P(l_N\mid l_{N-1}).
\]

\newpage
\subsection*{Entropy 2 (slide~\pageref{slide:model2})}

\subsubsection*{Conditional entropy}

On average, what is the expected information content of each additional letter
in the sequence drawn from the simple bigram ensemble?

It's $\log_2 \big[(1 / P(l\mid m)\bigr]$, averaged over $l$ and $m$ using $p(l,
m)$:
\[
\begin{aligned}
  H(X\mid Y) &= \sum_{x, y} P(x, y)\log_2\frac{1}{P(x\mid y)} \\
  &=  \sum_x\sum_y P(x, y)\log_2\frac{1}{P(x,y)/P(y)} \\
  &=  \sum_x\sum_y P(x, y)\log_2\frac{1}{P(x,y)} -  \sum_x\sum_y P(x, y)\log_2\frac{1}{P(y)}
  \\
  &= H(X,Y) - H(Y)
\end{aligned}
\]

\newpage
\subsection*{Second attempt (slide~\pageref{slide:model2})}
Recap: We want to find a permutation $\pi$ which minimises the information content
of ciphertext, given that~$pi$.

Can't seem to obtain the answer by inspection and there are too many permutations
to enumerate. Let's try search.

A \emph{transposition} is a permutation that merely exchanges two
elements. Every permutation can be written as a product of transpositions
(although not uniquely so). In fact, here I think each permutation can be
written as a product of at most 25 transpositions.

So here's the plan: Starting from some given permutation, search through the
$(26\times 25)/2 = 325$ permutations obtained by multiplying by a transposition. Replace
the permutation with the one that produces the lowest information content of the
associated plaintext or stop if there are none better than the current
permutation.

\newpage
\subsection*{Bayes (slide~\pageref{slide:bayes})}
The chain rule works both ways:
\[
p(x\mid y) p(y) = p(x, y) = p(y \mid x) p(x),
\]
from which \emph{Bayes' rule} is immediate:
\[
p(x \mid y) = \frac{p(y \mid x) p(x)}{p(y)}.
\]

You might wonder what is so interesting about this. However, there are
circumstances where the joint distribution is given to us precisely in the form
of the numerator, using the chain rule, and we wish to find the `other'
conditional. These circumstances are typically when the world is believed to
produce observational data (the `$y$'s) in a process that may be partly random
but depends upon some other data (that $x$'s) that we cannot observe or do not
know. This `generative model' is captured by the conditional $p(x\mid y)$. Our
`starting view' of $x$, the \emph{prior}, is captured by $p(x)$ and $p(x\mid y)$ is
our `updated view,' the \emph{posterior}.





\newpage
\subsection*{Introduction to Part 2}
Sampling from a discrete distribution is straightforward if you can sample
randomly from $[0,1]$. But what does it mean to sample from $[0,1]$? That cannot
be a sample from an ensemble. An ensemble would require a probability mass
function $p(x)$ such that $\sum_{x\in [0,1]} p(x) = 1$. But there are uncountably
many numbers between 0 and 1 so either the sum of the $p(x)$ is infinite or all
but countably many of the $p(x)$ are zero.

The solution to this problem is to consider not individual outcomes but sets of
outcomes. Instead of asking, `what is the probability of the sample being
0.782?' (which is zero) we ask, say, `what is the probability that the sample
lies in the interval $[0.5,1]$.

A set of outcomes---a subset of the sample space---is called an \emph{event}, and it
is events to which $P$ assigns a probability.

This explains a lot of the odd notation. Instead of $P(x = \text{a})$ we should
really have written $P(\{a\})$. And instead of, for example, $P(x>0.5)$ we should
write $P\bigl( \{x\in[0,1]\mid x>0.5 \}\bigr)$. And so on. 

However, in many cases not all subsets of the sample space can consistently be
assigned a probability, at least not if you want certain other desirable
properties to hold. That is, there are some subsets which don't get assigned a
probability at all---not even zero. The subject of \emph{measure theory} describes
all this.





\newpage
\section*{Appendix: Useful functions}

<<R_convenience_functions, eval=FALSE, echo=TRUE, highlight=TRUE, size="small", tidy.opts=list(width.cutoff=I(64))>>=
## Invert a permutation
perm_inverse <- function(p) {
    p[p] <- seq_along(p)
    p
}

## Compose two permutations
`%after%` <- function(p1, p2) {
    p1[p2]
}

## Convert text to and from numbers
encode <- function(txt) {
    factor(stringr::str_split_fixed(txt, "", Inf)[1,],
           levels = letters, ordered = TRUE)
}

decode <- function(cs) {
#    cat(as.character(cs), sep = "")
    paste0(as.character(cs), collapse="")
}

## Return a permutation as a single string
perm_as_string <- function(p) {
    paste0(letters[p], collapse = "")
}

## Encipher and decipher text, expressed as a factor
encipher <- function(key, plaintext) {
    factor(letters[key[plaintext]], levels = letters, ordered = TRUE)
}

decipher <- function(key, ciphertext) {
    factor(letters[perm_inverse(key)[ciphertext]], levels = letters, ordered = TRUE)
}

## (Slowly) compute the log-likelihood (base 2) of a text under the bigram ensemble
## This function is not optimised for speed (the mapply is the problem)
## cs: a vector of int (eg, produced by `encode`) of length at least 1
## bigram: a matrix of probabilities where bigram[x, y] means the probability of the pair yx
bigram_log_likelihood <- function(cs, bigram) {

    marginal <- log2(apply(bigram, MARGIN = 2, sum))
    normalisation <- apply(bigram, MARGIN = 1, sum)
    conditional <- log2(bigram / rep(normalisation, each = length(marginal)))
    
    Ncs <- length(cs)

    cdl_ll <- function(i, j) {
        conditional[i, j]
    }
    
    marginal[cs[1]] + sum(mapply(cdl_ll, cs[-1], cs[1:(Ncs - 1L)]))
}


## A faster version
## Precompute the marginal and conditional by:
##    marginal <- log2(apply(bigram, MARGIN = 2, sum))
##    normalisation <- apply(bigram, MARGIN = 1, sum)
##    conditional <- log2(bigram / rep(normalisation, each = length(marginal)))
bigram_ll_fast <- function(cs, log_marginal, log_conditional) {
    N <- length(cs)
    firsts <- as.integer(cs[1:(N-1)])
    seconds <- as.integer(cs[-1])
    marginal[cs[1]] + sum(conditional[26 * (firsts - 1) + seconds])
}

## Find the permutation "adjacent" to the give one that has the lowest information content, or FALSE
## if this is the best one perm : starting permutation suprise : function which takes a permutation
## and computes its surprise
minimise_ll_step <- function(perm, suprise) {
    current_surprise <- surprise(perm)
    N <- length(perm)
    
    x <- rep(1:N, each = N)
    y <- rep(1:N, times = N)

    xx <- x[x < y]
    yy <- y[x < y]

    guesses <- mapply(
        function(f, s) {
            guess <- perm
            guess[c(s, f)] <- guess[c(f, s)]
            guess
        },
        x, y, SIMPLIFY = FALSE)

    surprises <- lapply(guesses, surprise)

    best <- which.min(surprises)

    if (surprises[best] < current_surprise) {
        guesses[best][[1]]
    } else {
        FALSE
    }
}

@ 






\end{document}
